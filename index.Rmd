--- 
title: '経済学と反事実分析 接触篇 Economics and Counterfactual Analysis: A Contact'
author: "ill-identified"
date: "`r Sys.Date()`"
output: bookdown::gitbook
documentclass: bxjsreport
classoption: ja=standard, xelatex
link-citations: yes
bibliography: Structural.bib
site: bookdown::bookdown_site
biblio-style: jecon-custom
description: Tokyo.Rの続き
---

```{r include=FALSE}
# automatically create a bib database for R packages
knitr::write_bib(c(
  .packages(), 'bookdown', 'knitr', 'rmarkdown'
), 'packages.bib')
```

```{r setup, include=F}
require(knitr)
knitr::opts_chunk$set(
  echo = F, include=T,
  message=F, warning=F,
  out.width = "75%",
  digits = 3,
  cache=T,
  fig.align = "center")
run_heavy <- T #時間のかかる処理もやるかどうか
```
```{r setup2}
require(conflicted)
require(ggdag)
require(dagitty)
require(latex2exp)
require(skimr)
require(np)
require(plotly)
require(coefplot)
require(plm)
require(gmm)
require(tidyverse)
require(ggthemes)
require(Metrics)
require(SemiPar)
require(mgcv)
require(ranger)
# require(bkmr)
# require(gplm)
require(boot)
require(estprod)
require(patchwork)
require(stargazer)
conflict_prefer("lag", "dplyr")
conflict_prefer("filter", "dplyr")
conflict_prefer("TeX", "latex2exp")
conflict_prefer("spm", "SemiPar")
theme_set(new = theme_bw())
thm <- theme_classic(base_size = 20) + theme(legend.position = "bottom", axis.title.y = element_text(angle = 0, vjust = .5))

beta_a. <- 1.0
beta_k. <- 0.7
beta_l. <- 0.2
```

# 概要

第83回Tokyo.Rのエントリ. 前回の戦後処理に時間を取られたので, 手抜き回.

前回にも言及した @igami2018Artificial の研究に関連して, **構造推定**についてもう少し詳しい話をしようと思っていたが, 残り時間の問題から**動学構造推定**は難しいと判断した. しかしアンケート至上主義なので経済学のネタは外せないから, 準備の簡単な**静学構造推定**の話をとりあえずやることにした. 静学構造推定は「動学」ではないので時間変化を考えないタイプの理論モデルを推定する一連のテクニックである. 今回の話は比較的簡単かつ昔から知られている話だから大学の経済学部でも授業で取り上げられることもあろう. 知ってる人も多いかもしれない.

具体的には, 生産関数を構造推定するOP法[@OlleyPakes1996]をRでやる. 実はこれらは既に計算プログラムが`estprod`パッケージによって提供されているが, これはうまくいくかが怪しいのでOP法の意味を解説しつつ実装してみる.

プログラム全文等付属物はこちらにある
https://github.com/Gedevan-Aleksizde/20190125_tokyor

注意: 引用文献の名前などのフォーマットがおかしいが, これは対応したcslフォーマットが見つけられなかったため. そのうちなんとかする.

```{r cc-logo-html, echo=F, eval=knitr::is_html_output()}
knitr::asis_output('<a rel="license" href="http://creativecommons.org/licenses/by-nc-nd/4.0/"><img alt="クリエイティブ・コモンズ・ライセンス" style="border-width:0" src="https://i.creativecommons.org/l/by-nc-nd/4.0/88x31.png" /></a><br />この作品は<a rel="license" href="http://creativecommons.org/licenses/by-nc-nd/4.0/">クリエイティブ・コモンズ 表示 - 非営利 - 改変禁止 4.0 国際 ライセンス</a>の下に提供されています.')
```
```{r cc-logo-non-html, echo=F, eval=knitr::is_latex_output()}
knitr::include_graphics(here(dirname_img, "cc4.png"))
```
```{r cc-logo-text, echo=F, eval=!knitr::is_html_output()}
if(knitr::is_latex_output()){
  knitr::asis_output("`この作品は\\href{http://creativecommons.org/licenses/by-nc-nd/4.0/}{クリエイティブ・コモンズ 表示 - 非営利 - 改変禁止 4.0 国際 ライセンス}の下に提供されています.`{=latex}")
} else {
  knitr::asis_output("この作品はクリエイティブ・コモンズ 表示 - 非営利 - 改変禁止 4.0 国際 ライセンスの下に提供されています.")
}
```

# イントロダクション

## 構造推定とは

では構造推定とは何だろうか. 構造推定で検索すると, 名前が似ているが違うものがいくつか出てくる.

### 構造方程式モデリング

例えば構造方程式モデル(SEM)というものがある. これは変数間の関係構造を同時線形方程式で表したもので,
因果推論のフレームワークの原型ともいえるものだが,
**構造推定とは違う**.

### 構造時系列モデル

**構造ベクトル自己回帰モデル** (Structural VAR) とか, **ベイズ構造時系列モデル** (BSTS) というものがある.

通常のVARは複数の変数の時系列変化を扱うモデルで, それぞれの変数が過去の値にのみ依存して決まる回帰モデルなのに対して, 構造VARは現在の別の変数にも依存する, **同時決定的**な時系列モデルである[^stVAR]. ベイズ構造時系列モデルの「構造時系列」はHarveyのいう構造時系列モデルのことを指す[@ScottVarian2014]. これは89年とかなり古い教科書なので私も読んだことがないが, @lutkepohl2007New, p. 618によればローカルトレンドモデルを基本とした時系列モデルらしいので, 多変量時系列モデルというわけでもなく, 状態空間モデルに属する. これらも**構造推定とは関係ない**.

[^stVAR]: 簡単な定義と特徴は[https://qiita.com/saltcooky/items/2d0119ea4a10bab6cff2]の最後のほうに書いてある. より詳しい話を知りたければ, @okimoto2010 が言及している. インターネットの日本語圏で出回っている情報の多くはこのあたりからだと思うが, もっと多変量時系列モデルを知りたいなら @lutkepohl2007New の教科書がある (この本は約700ページの間, タイトル通りマジで多変量時系列モデルのことしか書いてない). 時系列モデルに関する本は @Kitagawa2005 の教科書を始め和書でも多く出ているが, 経済学や数理ファイナンスの観点から書いたもの以外では, 構造VARについて言及したものは私の知る範囲では見られない. 構造VARという考え方自体がパラメータの識別を前提としているからだろう. マクロ経済学における時系列モデルの利用を知りたいのなら, @watabe2016, @NakajimaWatabe2012 が参考になる. 本格的な教科書は専門外なので知らない.

### 構造型モデル

いちおう過去のTokyo.Rの発表ネタとかぶらないか確認したところ, こんなものを見つけた.

[Rで学ぶ『構造型モデル de 倒産確率推定』](https://www.slideshare.net/teramonagi/r-de)

「構造型モデル」という名前も構造推定と似ている. いちおう元になった @merton1974Pricing の論文も確認した. しかしこれも**構造推定ではない**.

構造推定の定義について, 多くの人は @REISS20074277 を挙げるが, この本はとにかく分厚くて高いので私は持っていない. 前回で触れたように,観察されたデータにそのまま回帰式や他の関数を当てはめても因果関係を知ることはできない. そこで, いくつかの要素が必要になる. そこで構造推定は仮定によって構造を特定しようとした.

北村 (2016, 応用ミクロ計量経済学の手法と論点)は一般的な型はないとしつつ, その解説を要約すると3つの特徴があるとしている.

1.  経済理論モデルであること

2.  確率モデルであること

3.  データに基づいて推定されていること

(1)はいわゆるルーカス批判 @Lucas1976 を受けて, 彼の批判を克服できるような経済理論に基づいたモデルづくりを意識するようになった.
(2)はちょっとわかりづらい. 回帰分析だって正規分布を暗に仮定しているから確率モデルになる. しかしここでは,
観察できないものと, できるものの不確実性を考慮しているという意味である. 例えば測定誤差や, 分析者には観測できないが,
経済活動の当事者は知っている情報というものがある.
これらを無視した推定はしばしばバイアスを引き起こすが,
構造推定ではモデルにこのような仮定を取り込む. (3) は, 完全な架空の話ではなく,
なるべく現実と整合するために観測されたデータに基づいてパラメータを決定するということである.

よって, 最初に挙げた「構造方程式モデリング」や「Mertonの構造型モデル」は経済理論に基づいた構造を表現していないため構造推定ではない.
ただし構造時系列モデルは, 実は80年代にマクロ経済モデルとして利用された歴史がある.
経済理論に基づかないという批判は当時からあったものの,
単に予測するだけなら問題ないということで使われていた. しかし, 「どうなるか」と「なぜそうなるのか」のどちらを求めているかと言えば前者であり,
原因を分析することはできない[^granger].

[^granger]: そのため, VARでも因果関係を定義できるということでGranger因果性が注目を集めた. つまり, 以前の投稿で問題にした, 予測か因果かという二択は, 経済学の発展のなかでもしばしば議論されてきた問題だということがわかる.

一般に定まった方法はないが, かといって完全な無手勝流ではない. 大学院のコアコースで教えるモデルはだいたい決まっており,
多くの研究も先行研究のフレームワークを応用していることが多い.

## 因果推論と何が違うのか

[以前にも紹介した](http://ill-identified.hatenablog.com/entry/2020/01/20/000406)ようなRCT, 準実験(操作変数法, 傾向スコア法, DID, RDD) なども, 「反事実的な因果推論」と呼ばれる. しかしこれらは経済学の理論仮説に基づく因果関係ではなく, ランダム化と比較によって「**平均処置効果**」を求めるフレームワークである[^ate]. 一方で構造推定は実際のデータと理論モデルを融合させた結果なので, 本来の意味で現実には起こり得なかったこと (反事実) を推論するフレームワークである.

具体例を挙げよう. 今回は経済学における**生産関数**の推定の例を紹介する. 原材料を投入し, 財を生産する, その変換ルールを関数に表したものを生産関数という. 例えば, **資本**($K$apital[^capital])と**労働力**($L$abor force), $Y$は出力, つまり生産量なので, 生産関数を以下のように表せる

$$\begin{aligned} Y= & F(K,L)\end{aligned}$$

この生産関数の形状が分かれば,

  - 実際のデータとの残差から, 数値に表れない生産性 (TFP) を計測する (残差分析)

  - もし設備投資を増やしたら ($K$を増加させたら) どれくらい生産量が増えるか

  - もしパラメータが変化したら, 生産力はどう変化するか (**比較静学分析**)

と言ったことがわかる.経済学部出身者ならば, ミクロ経済学の授業で聞いた覚えがあるだろう. しかし, データから推定した係数が「**もし変わったら**」というのは, 本質的に**現実に起こっていない事象**について話していることになる. 同等のことを因果推論でやるとすれば, 両方のケースについて**実際に観測したデータに基づいて差を比較**しなければならない. つまり, 単にデータを近似しただけの関数では, これらのことを言う根拠として弱い.

経済学ではRCTや準実験アプローチは**誘導形** (reduced form) 推定[^reduced]とも呼ばれ, **構造推定派**と**誘導形推定派**は数年前までお互い相手のアプローチの問題点を指摘し激しく論争していた. その結果, 現在ではそれぞれのアプローチでできることの限界がはっきりしてきたため, 論争というほどのことはない[^reducedvsstructural].

両者の使い分けとしては, 構造推定は経済理論モデルというある意味「架空のもの」に基づいて**未来に起こりうることについての分析**もできるが, 因果推論は実態としては実験計画法と本質的に同じで統計学の理論に基づいて観察できたデータから**事後評価**するアプローチである. よってしばしば, 構造推定は**反事実**(**反実仮想**)**シミュレーション**[^souiyeba], 因果推論は**プログラム**(**政策**)**評価**とか**介入**(*intervention*)といって呼び分けられる[^intervention]. そして構造推定は理論仮説に強く依存したフレームワークであるので, 仮説が妥当なのかはよく注意が必要である. ただし, かといって因果推論が仮定に依存しないというのも完全な誤りである. 前回でも言及したが, 因果推論であっても必ず仮定が存在するので, それを無視して計算した平均値の差が因果効果を意味するとは限らない.

[^capital]: ドイツ語でのスペル. $C$にしてしまうと消費, Comsumptionと重複するからか, 昔からこの記号が使われる. また, 正確には「資本ストック」であり,
    設備投資や固定資産などから計算する
[^ate]: 正確には少し違うが, 重要なのはサンプルの平均的な差を見るという特徴が共通している
[^reduced]: 構造推定に使うような, 経済現象の構造を表したモデルを, 平均の差の比較という因果推論のフレームワークで推定できるように単純な形に変形することからこう呼ばれる. この式変形のため, 根本的な構造パラメータを特定しづらいという問題がある.
[^reducedvsstructural]:  歴史的経緯は @Kitamura2016 などが少しだけ解説している. 計量経済学の観点では統計モデルとして @hayashi2000Econometrics, Ch. 8 で言及している. また, @CameronTrivedi2005, Ch. 2 は教科書の最序盤で構造モデルと誘導モデルの特徴付けを行うという独特の構成になっている.
[^souiyeba]: そういえばこの用語で呼び始めたのは一体誰なのだろうか? たぶんRubinではない.
[^intervention]: 後者の因果推論フレームワークも, もともとは counterfactual causal modelと呼ばれていたが, (おそらくはこの性質の違いのため)少なくとも経済学の分野では最近はあまり強調されていない気がする.

## 今回取り組む問題

動学構造推定はテクニカルでなんかすごいことをやってるという印象を簡単に与えられる. しかし, 今回は時間がないのでもう少し簡単にできるほうをやる.
前回話題にしたのは「動学的」構造推定であり, 時間変化を取り入れたモデルを扱うジャンルである. 一方で, 今回は考慮しない「静学的」構造推定の具体例を紹介する. といっても昔からある研究なので, すでに日本語の講義スライドもネット上に転がっていたりするし, もしかしたら発表者の中にも授業でやったことがあると言う人がいるかもしれない.

今回は特に, 生産関数の推定の問題を例にする. 生産関数をどういう形にするかはいろいろだが, よく使われるものの1つに コブ=ダグラス型生産関数がある[^cobbd]. 特に, 一企業の生産関数を,

$$\begin{aligned}Y=f(K,L):= & \alpha K^{\beta_{K}}L^{\beta_{L}}\end{aligned}$$

という指数関数で表す[^prodfunc]. $\alpha,\beta_{K},\beta_{L}$ はパラメータである. $\beta_{K}=\beta_{L}=0.5$ の場合, 図\@ref(fig:cobbdouglas) のような形状になる.)

```{r cobbdouglas, fig.cap="コブ=ダグラス生産関数の形状"}
include_graphics("img/cobbdouglas1.png")
```


[^cobbd]: 他にも色々あるが, 例えば物理学のように法則がかなり明確になっている分野を勉強した人間からはおかしなことのように聞こえるかもしれない. しかし, 経済学ではむしろ, 変に特定化すれば特定化の誤りを免れない.
[^prodfunc]: 生産関数のより詳しい数学的性質はミクロ経済学の教科書に書かれている. 例えば, 特に有名なのは @nishimura1990, 『ミクロ経済分析』[@varian1992Microeconomic], @MWG などである. それ以降にも評価の高いミクロ経済学の和書がいくつかでているが, 私はあまり読んでいないのでなんとも言えない.

# どう推定するか

経済学部の計量経済学の講義では, **回帰分析**で生産関数の推定をやるところも多いと思う. 前節の生産関数は, $Y$を**被説明変数**, $K,L$ を**説明変数**としても線形式ではない. しかし, $Y,K,L$がいずれもゼロ以下になりえないことに着目すると, 対数を取って以下のようになる(いわゆる**対数線形回帰モデル**).
$$\begin{gathered}\ln Y=\ln\alpha+\beta_{K}\ln K+\beta_{L}\ln L\end{gathered}$$
となる. つまり, **おなじみの最小二乗法**(OLS)**で計算できる**. 観測できなかった$\alpha$も, 切片パラメータ として推定できることになる. 以降はそれぞれの対数を$y,\beta_{A},k,l$に置き換えて,
$$\begin{aligned}
y &= \beta_{A}+\beta_{K}k+\beta_{L}l+\varepsilon\end{aligned}(\#eq:prod-log-linear)$$
という線形回帰モデルを考える. しかし以前書いたように, 経済学の文脈では非線形だろうが線形式だろうがデータに関数を当てはめただけでは用をなさない. 経済理論に即して, 発生しうる問題を排除した方法で推定しなければならない. つまり, 現在ではOLSでは生産関数のパラメータを**推定するには不十分**と考えられている.

発表時間の問題から, スライドでは大幅に省略しているが, ここでは冗長気味にパネルデータ分析の発展の経緯も合わせて説明していく[^panel].

現在の経済学の観点からすると, \@ref(eq:prod-log-linear)をそのままOLSで推定するのをためらう大きな理由は以下の3つである.

1.  企業特有の効果が存在する

2.  需要要因を考慮していない

3.  セレクションバイアス

[^panel]: こういった初歩的な話は, 私なんかよりよほど深く理解した大学の先生がいくつも教科書を書いており, 正確さや詳しさでそちらのほうが遥かに良い. 参考文献には一般公開されている @Okui2016 のスライド, @Wooldridge2010panel, Ch. 10-11 の教科書などがある. 私はほぼWooldridgeの教科書で勉強したため, 日本語の教科書に詳しくない……

## なぜ生産関数を知りたいのか

しかし, テクニカルなことを説明する前に, そもそもなぜこのようなことをするのかを表明したほうがよかろう. 今回, 最後に紹介するのは @OlleyPakes1996 の研究で, 彼らは法規制が産業の**生産性**にどう影響するかを知ることを研究目的としていた. では, 生産性なるものをどう計測すればいいのだろうか.

経済学には**全要素生産性** (*TFP; total factor productivity*)という概念がある. これは, **投入したリソースの量では説明できない生産量の多さ**を生産性と定義した概念で, 実際には推定した生産関数による生産量の予測値 (理論値) と実測値の誤差で計測する . 例えば今回はコブ=ダグラス型関数で, モデルの仮定から, 物的・人的リソースによる生産量は$K^{\beta_{K}}L^{\beta_{L}}$の部分になる. よってTFPを計算する式は,
$$\begin{aligned}\mathit{TFP}:= & \frac{Y}{K^{\beta_{K}}L^{\beta_{L}}}\end{aligned}$$
となる. もしTFPを毎年計測して値に変化があるのなら, それは**技術進歩**に由来する生産性の向上であると見なせる(正確にはこれは**ソロー残差**とか**成長会計モデル**とか呼ばれる)[^solowresid].

さらに, \@ref(eq:prod-log-linear)からも分かるように, 対数の性質には $\ln(x/y)=\ln x-\ln y$というものがある(高校数学でも扱うはずだ). よって, 仮に\@ref(eq:prod-log-linear)の係数パラメータ$\beta_{K},\beta_{L}$を求められれば,
$$\begin{aligned}
\ln(\mathit{TFP}):= & y-\hat{\beta}_{K}k-\hat{\beta}_{L}l\\
\therefore\mathit{TFP}= & \exp(y-\hat{\beta}_{K}k-\hat{\beta}_{L}l)\end{aligned}$$
となる. よって回帰分析で推定した式を使うと, TFPは$\alpha$だけでなく誤差項も含まれることになる. TFPは回帰モデルの**残差に依存する**.

ではここで, 当てはまりの良さのために生産関数の形状を変えたらどうなるだろうか? 当てはまりを良くするのなら, いろいろなモデルを試していくらでも生産量の予測誤差の少ないモデルを作り出すことができる. しかしそこから計算した残差は**一体何を意味するのだろうか**? 機械学習や統計モデリングというと, MSEとか対数損失がどれくらい小さいかというのがモデルの良さの指標だと語られることがしばしばあるが, しかしこのようにそれが活用方法としての全てではない.

そしてTFPや生産性といった概念は, このように生産構造をどう仮定するかに強く依存することに注意する. 分析にはかならず「**ある仮定のもとで...**」という前置きが存在する. 「生産性」という抽象的な概念が何を意味するのか, 何を言っているのかはよく考えるべきであろう[^residual-analysis].

[^solowresid]: TFPの計測方法やその批判についてはググれば内閣府とか日銀とか経済産業研究所 (RIETI) の公開している日本語の論文がたくさん引っかかるので適当に勉強すれば良いだろう.
[^residual-analysis]: 私が過去, 残差を使って何かを言おうとする「データ分析」のやり方に対して, 例えば『[CausalImpact でできること, できないこと](http://ill-identified.hatenablog.com/entry/2019/10/09/120000)』で書いたように注意を喚起してきたのはこのように,
    強い仮定に依存しているにもかかわらず仮定の妥当性を確認せずに濫用されることを懸念したのが理由である.

## パネルデータ分析と観測されない効果

「企業特有の効果」とは, 特定の企業だけが「特殊な技術を持っている」「優秀な人材が集まっている」など, 単純な量だけでは捉えきれない情報を意味する.
つまり, 企業ごとに異なる$\omega_{i}$が生産量に影響していると考える.
$$\begin{aligned}
y_{i}= & \beta_{A}+\beta_{K}k_{i}+\beta_{L}l_{i}+\omega_{i}+\varepsilon_{i}\end{aligned}$$
$\omega_{i}$は観測できないので, このまま $(y_{i},k_{i},l_{i})$のデータで回帰すると, $\omega_{i}$は定数項$\beta_{0}$または誤差項$\varepsilon$に吸収される. $\omega_{i}$が$k_{i},l_{i}$と全く相関しない変数ならば, $\beta_{A}$と$\omega_{i}$を識別できないものの, 少なくとも$\beta_{K},\beta_{L}$の推定はできる.

このような問題をどう解決すれば良いだろうか? まずは問題1だけを考えよう. $\omega_{i}$は観測できないが, では推定できないだろうか? つまり, 同一の企業の情報を何度も観測しすれば, そこから推定できないか, というアイディアである. これが**パネルデータ**分析のスタート地点になる. パネルデータとは, 同一個体を追跡して複数時点で観測したデータである(表\@ref(tab:panel-example)).

```{r panel-example}
df_panel <- tibble(
  `企業ID` = c(rep("001", 4), rep("002", 2)),
  `時期` = paste0("2019Q", c(1:4, 1:2)),
  `生産量` = c(100, 110, 90, 120, 80, 90),
  `資本` = c(10, 11, 12, 14, 5, 6),
  `労働力` = c(10, 10, 12, 13, 5, 5)
)
kable(df_panel, caption="パネルデータの例", booktabs=T)
```


よって, 企業$i=1,\cdots,N$ごとに, $t=1,\cdots,T$の観測がある. そこで以降は回帰モデルを以下のように書く.

$$\begin{aligned}
y_{i,t}= & \beta_{A}+\beta_{K}k_{i,t}+\beta_{L}l_{i,t}+\omega_{i}+\varepsilon_{i,t}\end{aligned}(\#eq:panel-basic)$$

ここではひとまず, $\omega_{i}$が企業$i$ごとに異なるが, 時間$t$に対しては変化しないと考えておこう. また, 問題1だけを考えるにあたって, $\omega_{i}$は$k_{i,t},l_{i,t}$と**相関しない**としておこう. このような$\omega_{i}$を**観測されない個別効果**(unobserved indivisual effect) という.

さらに追加の仮定として, $\omega_{i}$が定数である場合, **ダミー変数**を作成することでOLSであるかのように$\omega_{i}$を推定できる[^robust-se]. これを**LSDV推定量**という. また別の解き方としては, \@ref(eq:panel-basic)を企業ごとに平均を取った場合を考える.
$$\begin{aligned}
\bar{y}_{i}= & \beta_{A}+\beta_{K}\bar{k}_{i}+\beta_{L}\bar{l}_{i}+\bar{\omega}_{i}+\varepsilon_{i,t}\end{aligned}(\#eq:panel-FE)$$

このとき, $\omega_{i}$は企業ごとに一定なので, $\bar{\omega}_{i}=\omega_{i}$である. よって, \@ref(eq:panel-basic)から\@ref(eq:panel-FE)を引いてからOLSで計算することでも同じ推定結果が得られる.
こちらの方法は**固定効果**(*FE; fixed effect*)推定量または**グループ内**(*within*)推定量と呼ばれる[^panel-between].

一方で, $\omega_{i}$が定数項ではなく確率変数の可能性もある. つまり, 企業ごとに異なる$\omega_{i}$は**実は期待値をとればゼロになる確率変数にすぎず**, 定数項に見えたものは確率的なゆらぎに過ぎないかもしれない. この仮定の元に考案されたのが**変量効果**(RE; random effect)モデルである. REモデルは数学的に**分散不均一**な回帰モデルと同一のため, 実質的に**一般化最小二乗法**(*GLS*)と同じである[^glmm]..

[^robust-se]: ただし, 一致するのは期待値のみで, 標準誤差の計算方法は変わる. 仮説検定・区間推定をするならば *clustering-robsut*な標準誤差と呼ばれるものを使う.
[^panel-between]:  逆に観測されない効果が時間ごとに異なるが, 企業間では一定の場合も同じようにできるだろう. その場合は**グループ間** (*between*) 推定量と呼ばれる.
[^glmm]: @Kubo2012 は生物学の文脈で異質効果のある場合の解法として**一般化線形混合モデル**(GLMM)を紹介している. これは観測されない効果の問題とよく似ている. 生物の実験データではおそらく変数との相関がないため, 定数項のバイアスと, 当てはまりの良さだけが問題となるのだろう. GLMMは誤差分布が正規分布と異なる場合も考えているが, パネルデータは線形モデルを基本として議論を進めることが多い.

## 動学パネルデータ分析

このように, 観察できない要因があってもパネルデータがあれば理論上は推定が可能になる. しかし, @Mairesse1990 [^mairesse]はこれまで紹介した方法とほぼ同じやり方で企業の生産関数を推定したところ, 観測されない効果を識別しようと複雑に差分を取れば取るほど, 係数の推定値が小さくなることを発見した. 彼はこの原因の仮説をいくつか挙げている.

1.  内生性の問題. 個別効果$\omega_{i}$は物理的なものではなく, 環境, 経営者の手腕といったものに由来するのなら, 他の説明変数$k,l$の決定に影響している.

2.  企業の意思決定のラグ. 経営者が誤差項として現れる短期的な変化, つまり直前の$\varepsilon_{i,t-1}$に対応して$k_{i,t},l_{i,t}$を決定しているなら,
    誤差項と説明変数が相関しており, 単純な差分では推定できない.

3.  同時決定の問題.

4.  測定誤差. 各変数に測定誤差があれば, 差を取ることで打ち消される.

これらの問題への対応として考えられるのが, **操作変数**である. \@ref(eq:panel-basic)に, 被説明変数$y_{i,t}$の自己回帰項を追加する.

$$\begin{aligned}
y_{i,t}= & \beta_{A}+\beta_{K}k_{i,t}+\beta_{L}l_{i,t}+\rho y_{i,t-1}+\varepsilon_{i,t}\end{aligned}(\#eq:panel-dynamic)$$

こういう自己回帰項を含むモデルを扱う分野はパネルデータ分析のなかでも特に**動学パネルデータ分析**と呼ばれる. このあたりの問題も @Okui2016 や @Wooldridge2010panel, Ch. 11 で丁寧に説明されている[^dynamic-panel]ので冗長だが, 一応簡単に述べておく.

前節のアイディアを流用すれば, Within推定量の応用で時間平均$\bar{y}_{i}=(T-1)^{-1}\sum_{t=2}^{T}y_{i,t}$を取ることが考えられる. しかし, パネルデータでは$N$が大きいが$T$は小さいことが多いため, 一致推定量とみなせない. 一方で, **1階差分**(*FD*) 推定量というものがある. これはある意味**DID推定量**と同じである. 差分演算子を $\Delta x_{t}:=x_{t}-x_{t-1}$というふうに定義する. \@ref(eq:panel-dynamic)の両辺から$y_{i,t-1}$を引くと, 以下のようになる.
$$\begin{aligned}
\Delta y_{i,t}= & \beta_{K}\Delta k_{i,t}+\beta_{L}\Delta l_{i,t}+\rho\Delta y_{i,t-1}+\Delta\varepsilon_{i,t}\end{aligned}$$
しかし, $\Delta y_{i,t-1}=y_{i,t}-y_{i,t-1}$から, $\Delta\varepsilon_{i,t}=\varepsilon_{i,t}-\varepsilon_{t-1}$と相関が存在する. そこで, $\varepsilon_{i,t-1}$より過去の$y_{i,t-2},\cdots$を**操作変数**として使う方法が提案されている. それが @AndersonHsiao1981;@AndersonHsiao1982 モデルや, @ArellanoBond1991, @Blundell1998 のGMM推定法である(いわゆる**階差GMM**と**システムGMM**). しかし, これらは実際に操作変数として意味のある変数なのかということを深く議論せずに$\{y_{i,t-2},\cdots\}$を操作変数に利用している. その場合, 上記で挙げたような意思決定が存在するかどうかを検証できないし, これらの推定量には過剰識別や**弱相関操作変数**の問題が発生することが指摘されており, 妥当性に疑問が残る.

[^mairesse]: 古い本なので大学図書館などでないと閲覧が難しそうだ. 私も手元にないので学生の頃に書いたメモを元に書いている. また, 川口先生の講義ノートではこのあたりの問題に関する参考文献として @griliches1999Production を挙げている. タイトルからしてもこちらのほうがカバー範囲の広いサーベイのようなので, こちらを参照したほうがいいのかもしれない.
[^dynamic-panel]: GMMも以前[ブログ](http://ill-identified.hatenablog.com/entry/2015/02/22/203650)に定義を書いたが, 詳しいことは @hayashi2000Econometrics, また, 他の教科書として動学パネルデータを重点的に解説した @ChigiraEtAl2011 などがある. Baltagi の教科書も授業でおすすめされていた気がするが私は読んでいないのでどこまで書いてあるか知らない. どうしても本を買いたくない/買えない人 @Okui2016 の講義スライドを見ると良い.

## 企業内部の意思決定を考える

しかし, 経済学の理論では, 観察される経済データは市場が需要と供給のバランスを取った結果である. これが2つ目の「需要要因を考慮していない」という話になる. 企業が需要を全く考えずに事業計画を立てるとは考えにくいため, この$\omega_{i}$が間接的に$k_{i},l_{i}$に影響している可能性は十分ある. また, 企業は新規立ち上げだったり倒産したりする. 現実のパネルデータはしばしば欠落がある. そのようなデータでそのまま推定しようとしたらどうだろうか.

このようなパネルデータでの内生性に対処する推定方法に関する研究はいくつもあり, 特によく出てくるものとしてFEIVモデル, がある.

さらに, @OlleyPakes1996, @LevinsohnPetrin2003 はこれらの動学パネルデータ分析の方法から一歩進んで, 企業の意思決定の構造をモデル化した生産関数を考え[^firm-unit], 操作変数を使わずに内生性の問題に対処した方法を提案している (一般にはOPの方法は**コントロール関数**アプローチと呼ばれる)[^op-intro].

そこで今回はOlleyらのフレームワークを詳しく説明する. なお, 以降は\@ref(eq:panel-basic)式をもとに説明するが, 各企業で同じことが言えるため$i$を省略して書く. OPでは, 観察されない効果$\omega_{t}$は企業の経営者は知ることができるが, データとして現れないため分析者には観測できない, 企業固有の生産能力を表す成分である. $\omega_{t}$は他の変数の影響を受けない(外生的)が, 自身の過去の値からは影響を受けると仮定する. つまり1次の**マルコフ過程**であるとする.
$$\begin{aligned}
\omega_{t}\sim & P(\omega\mid\omega_{t-1})\end{aligned}$$

$t$期の始めに経営者は$\omega_{t}$を観測して, これをもとに今期の経営計画を決定, つまり, 純キャッシュフローの割引現在価値を最大化できるように$k_{t},l_{t}$を決めていると仮定する. そこで, 動学的なモデルを想定する. 企業の$t$時点現在での収益を$\pi_{t}(\omega_{t},k_{t},l_{t})$として, 投資$\mathit{inv}_{t}$に対する費用を $c(\mathit{inv}_{t})$とする. 経営者が最適化する現在の**価値関数** (*value function*)を $V_{t}(\omega_{t},k_{t},l_{t})$とすると[^age-omitting], 動学モデルでは単に現在の価値関数を最大化することが最適化に繋がるとは限らない. 経営者は, これらを把握した上で, 割引現在価値を最大化するように行動する. つまり, 以下のような $V^{\ast}$の達成を目指す.
$$\begin{aligned}
V^{\ast}= & \max_{\{k_{t},l_{t}\}}\sum_{t=0}^{\infty}\delta V_{t}(\cdots)\end{aligned}$$

これは無限和なのでそのまま計算するのは難しいが, 一定の条件下では上記の最適化問題の解が以下\@ref(eq:bellman)のような**ベルマン方程式** (*Bellman equation*) の解と同じであることが分かっている[^ft-bellman].

$$\begin{aligned}
V_{t}(\omega_{t},k_{t},l_{t})= & \max_{k_{t},l_{t}}\left\{ \Phi,\sup_{i_{t}\geq0}\pi_{t}(\omega_{t},k_{t},l_{t})-c(\mathit{inv}_{t})+\delta\mathrm{E}\left[V_{t+1}(\cdots)\right]\right\}
\end{aligned}(\#eq:bellman)$$

ベルマン方程式が意味するところは, $t,t+1$という**2時点間の関係さえ分かれば上記の無限和の問題も解ける**ということである.

ただし, このモデルでは2つの仮定を追加していることに注意する. 1つは, \@ref(eq:bellman)には, $\Phi$という選択肢があることだ. これは退出, つまり**工場の閉鎖**や**事業を売却**することで得られるキャッシュフローである. よって, このモデルでは採算が合わないと判断した経営者が事業撤退を選択する可能性も考慮している. もう1つは, 動学的な決定の必要があるのは$k_{t}$のみで, $l_{t}$は**毎期独立して決定される**ように仮定している点である..

では, $k_{t}$の動学的な意思決定がどのようになされるかを考えていく. 資本$k_{t}$は, 前期のストック$k_{t-1}$と, 減価償却率 $\delta$, そして新規の投資額 $\mathit{inv}_{t-1}$の和で表現できる.

$$\begin{aligned}
k_{t}= & \mathit{inv}_{t-1}+(1-\delta)k_{t-1}\end{aligned}$$

企業の退出ルールと投資関数を以下のように定義できる. $t$期に企業が存続するなら$x_{t}=1$, そうでないなら$0$として, $x_{t}$は以下のように決定される.
$$\begin{aligned}
x_{t}= & \begin{cases}
1 & \text{if }\omega_{t}\geq\underline{\omega}_{t}(k_{t})\\
0 & \text{otherwise}
\end{cases}\end{aligned}$$
$$\begin{aligned}
\mathit{inv}_{t}= & \mathit{inv}_{t}(\omega_{t},k_{t})\end{aligned}(\#eq:investment)$$
ここから, 企業が$t$期も退出せず操業を続ける確率は
$$\begin{aligned}
p_{t}:= & \mathrm{P}(x_{t}=1\mid\underline{\omega}_{t}(k_{t}))\\
= & \mathrm{P}(x_{t}=1\mid k_{t},\omega_{t},\underline{\omega}_{t})\end{aligned}(\#eq:exit-prob)$$
のように$k_{t},\omega_{t},\underline{\omega}_{t+1}$の条件に依存することを覚えておこう[^x-lag-notation].  退出による利益$\Phi$は観測できず, 別の変数で表現する必要があるため, 退出を決定するしきい値を表す関数$\underline{\omega}_{t}(\cdot)$を導入する. 

利潤関数$\pi_{t}$が$k_{t}$に対して減少関数なら, 価値関数にとっては増加関数, そして資本ストックの多い企業は一時的な生産性の変化$\omega_{t}$とはあまり関係なく, 将来に比較的大きな収益が見込まれるから, しきい値$\underline{\omega}_t(\cdot)$は比較的低くなるはずなので$\underline{\omega}_{t}(\cdot)$は減少関数になる(退出確率$p_{t}$の減少)はずだ. よってこの退出ルールから$\omega_{t+1}$に$x_{t+1},k_{t+1},l_{t+1},\omega_{t}$を条件付けた$\omega_{t+1}$の期待値
$$\mathrm{E}\left[\omega_{t+1}\mid k_{t+1},\omega_{t},x_{t+1}=1\right]$$
は$k_{t+1}$に対して減少関数であるとわかる ($l_{t+1}$は独立しているので$\omega_{t+1}$に影響しない). これが観測できない生産性 $\omega_{t}$と資本$k_{t}$が相関していることの理由付けになる.

ここまでで登場した変数間の関係をグラフィカルモデルで描くならば, 図\@ref(fig:op-graphical)のようになる.

```{r op-graphical, fig.cap="OPモデルで仮定した変数の関係"}
graph <- dagify(omega_f ~ omega_b,
                inv ~ omega_b,
                k_f ~ k_b + inv + omega_f,
                x ~ k_f + omega_f,
                Exit ~ x,
                Func ~ x + k_f,
                Func ~ l,
                y ~ Func)
coordinates(graph) <- list(
  x = set_names(c(2, 2, 0, 0, 1, 1, 0, 1, 1.5, 2.5),
                c("Exit", "Func", "inv", "k_b", "k_f", "l", "omega_b", "omega_f", "x", "y")),
  y = set_names(
    c(.5, 1.5, 1, 2, 2, 3, 0, 0, 1, 1.5),
    c("Exit", "Func", "inv", "k_b", "k_f", "l", "omega_b", "omega_f", "x", "y")
  )
)
df_graph <- graph %>% tidy_dagitty() %>%
  arrange(name)
df_labels <- tibble(name = df_graph$data$name %>% unique) %>% mutate(
  param = c("Exit", "f(k, l)", "\\mathit{inv}_t", "k_t", "k_{t+1}", "l_{t+1}", "\\omega_t", "\\omega_{t+1}", "x_{t+1}", "y_{t+1}"),
  time = as.factor(c(1, 1, 0, 0, 1, 1, 0, 1, 1, 1))
)
df_graph$data <- df_graph$data %>% left_join(df_labels, by = "name") %>% as.data.frame()
df_graph$data <- df_graph$data %>% mutate(edge_value = if_else(name == "x" & to == "Func", "1", ""))
df_graph$data <- df_graph$data %>% mutate(edge_value = if_else(name == "x" & to == "Exit", "0", edge_value))
df_graph %>%
  ggplot(aes(x = x, y = y, xend = xend, yend = yend)) +
  geom_dag_point(aes(color = time), show.legend = F) +
  # geom_dag_edges(aes(label = edge_value)) +
  geom_dag_edges() +
  geom_dag_text(parse = T, aes(label = latex2exp::TeX(paste0("$", param, "$"), output = "character"))) +
  annotate(geom = "text", x =  c(1.7, 1.7), y = c(.7, 1.1), label = c("0", "1")) +
  scale_color_colorblind() +
  scale_x_continuous(breaks = c(0, 1), labels = c("t", "t+1")) +
  theme_dag_blank() + theme(
    plot.background = element_rect(fill = "transparent"),
    panel.background = element_rect(fill = "transparent",color = NA),
    axis.line.x = element_line(),
    axis.text.x = element_text(),
    axis.ticks.x = element_line())
```



[^firm-unit]: より正確には, Olleyらは工場単位でのデータを利用しているため彼らが推定したのは工場単位での生産関数である.
[^op-intro]: 日本語では, たとえば北村, 西脇, & 村尾 (2009, *不完全資本市場下での生産関数の推定について*)が理論的根拠を詳しく解説している. 彼らの解説で使われているのは本来のものに加えて原材料費や純資産を考慮したモデルだが, 本質的な部分は変わらない. 統計モデルとしての説明なら彼らのものでも十分だが, 今回は経済モデルとの関係を強調したいため, あえてOlleyらの論文をなぞって冗長に書いておく.
[^age-omitting]: Olleyらの当初の仮定では, 企業年齢も生産性に影響があると考えていたが, 実際に推定してみるとほとんど影響がないことがわかった. モデルの特徴を説明する上でも企業年齢はあまり関与しないため, ここでは省略している.
[^ft-bellman]: これを**ベルマンの最適性原理**という. しかしこれ以降の動学最適化問題の数理的に厳密な話をしていてはキリがないので, 適当な参考文献に投げることにする. ただし私は動学最適化を純粋に数理的側面で説明した教科書を読んだことはなく, 経済学の文脈で主に勉強しているので, 挙げる参考文献も必然的にそういうものになる. 日本語文献でかつ無料で一般公開されているものとしては, マクロ経済学での応用の観点で書かれた工藤 (2007, *動学的最適化入門*), 阿部 (2017, *上級マクロ経済学 講義ノート 動的計画法*), 蓮見 (2019, *動学マクロ経済学入門*)の講義ノートがある. 教科書ではA.C. Chang の『動学的最適化の基礎』の邦訳は比較的簡単に解説しているが誤植が多いことで有名である. 西村清彦の『経済学のための最適化理論入門 』という教科書もあるが, どこまでカバーしていたかの記憶が定かではない. もっとも詳しいのはやはりStokey, Lucas, & Prescott (1989, *Recursive methods in economic dynamics*), Ljungqvist & Sargent (2018, *Recursive macroeconomic theory*)の教科書あたりか? 単に「ベルマン方程式」でググると強化学習に関連するものとして言及する記事がよく引っかかるが, これらはベルマン方程式というより分枝限定法の考え方の解説だったり, 内容がメモ程度の断片的だったりするので今回の話の補足説明には向かない. 理論モデルでは連続時間を想定する場合もあり, ベルマン方程式の代わりに Hamilton-Jacobi-Bellman方程式を応用することもある. こちらは物理学でもよく使われるようだ.
[^x-lag-notation]: 元の論文では時間インデックスをずらして$p_{t}:=\mathrm{P}(x_{t+1}=1\mid\cdots)$としていることに注意.

## OPモデルの回帰モデルとしての意味

経営者が (分析者には) 観察されない効果$\omega_{t}$を参照して$k_{t},l_{t}$を決めてしまうというのは, 計量経済学の文脈でいうなら欠落変数バイアスであり, ここまでで紹介したパネルデータの方法で推定してもバイアスが発生してしまう. さらに $\omega_{t}$のマルコフ性を仮定しているため, もし$\omega_{t}$が過去の値と相関しているならば, やはり内生性によるバイアスが発生する. また, OPモデルは経営者の撤退という選択肢も想定していることは, **サンプルセレクションバイアスを考慮している**とも言える. 前節ではパネルデータには一切欠測がない(**バランスドパネル**)ことを暗黙の前提としていたが, 実際のパネルデータではしばしば欠測が起こる. もし欠測が企業の活動とは完全に無関係にランダムに発生している(MAR; missing at random)ならば推定に大きな影響はないが, すでに解説したように$k_{t}$は$\omega_{t}$(の期待値)に相関し, $\omega_{t}$の大きさ次第で企業経営者は撤退を決める. つまりモデル内の変数に依存するメカニズムで発生している(MNAR; missing not —)ことになるので推定にバイアスを引き起こす[^missing-bias].

推定のため, 投資を$\mathit{inv}_{t}=\mathit{inv}(k_{t},\omega_{t})$という関数で表現できるとして, $\mathit{inv}_{t}>0$と仮定すると, 投資関数 $\mathit{inv}_{t}(\cdot)$が厳密に単調増加関数であれば[^investment-monotone], 逆関数 $\omega_{t}=h_{t}(k_{t},\mathit{inv}_{t})$ が存在することになる. 言い換えるなら, $\mathit{inv}_{t}$は**操作変数**である. これを\@ref(eq:panel-basic)に代入すると,
$$\begin{aligned}
y_{t}= & \beta_{L}l_{t}+\left[\beta_{A}+\beta_{K}k_{t}+h_{t}(k_{t},\mathit{inv}_{t})\right]+\varepsilon_{t}\end{aligned}(\#eq:OPmodel)$$
となり, **観察されない効果を他の説明変数で表現できる**ことがわかった. $\left[\cdots\right]$の部分は関数形が特定されていない, いわゆるノンパラメトリックモデルである. しかし, 今知りたいのは$\beta_{A},\beta_{K},\beta_{L}$であって, 投資関数を特定することは必須でない. そこで, OP法では以下のような**2段階の推定法**で一致推定量を得る.

**第1段階**では$k_{t}$と$\omega_{t}$という動学的に決定される部分をノンパラメトリック回帰することで, 静学的に決まる$\beta_{L}$のみを識別する. そこで, 相関のある部分をまとめて, 以下のように $\phi_{t}$と表す.
$$\begin{aligned}
\phi_{t}(k_{i,t},\mathit{inv}_{i,t}):= & \beta_{A}+\beta_{K}k_{i,t}+h_{t}(k_{i,t},\mathit{inv}_{i,t})\end{aligned}(\#eq:OP-partial)$$
$\phi_{t}$を使うと\@ref(eq:OPmodel)は以下のように書き換えられる.
$$\begin{aligned}
y_{t}= & \beta_{L}l_{t}+\phi_{t}(k_{t},\mathit{inv}_{t})+\varepsilon_{t}\end{aligned}(\#eq:OP-1st)$$ 
このモデルの$\phi_{t}$の部分をノンパラメトリック回帰で推定する[^nonpara]ことで, 推定値$\hat{\beta}_{L}$, $\hat{\phi}_{t}$が識別された. これは別の見方をすれば**部分線形モデル** (partially linear —) である[^cf].

$\hat{\beta}_{L}$を得られたので, 当初の式は以下のように書ける.
$$\begin{aligned}
y_{t}-\hat{\beta}_{L}l_{t}= & \phi_{t}(k_{t},\mathit{inv}_{t})+\varepsilon_{t}\end{aligned}(\#eq:op-model-reduced)$$

よって残りは$\omega_{t}$や$k_{t}$をひとまとめにした$\phi_{t}$からどうやって$\beta_{A},\beta_{K}$を識別するかという問題である. ここからがOlleyらの研究の肝であり, 込み入った説明になる箇所である. $\hat{\phi}_{t}$には$\beta_{A}+\beta_{K}k_{t}+\omega_{t}$が含まれているが, $k_{t}$は既に説明したように動学的に決定される. ここで, 次の$t+1$時点の $y_{t+1}-\beta_{L}l_{t+1}$の期待値を考えると, $\omega_{t}$のマルコフ性から以下のように書ける.
$$\begin{aligned}
\mathrm{E} & \left[y_{t+1}-\beta_{L}l_{t+1}\mid k_{t+1},x_{t+1}=1\right]\\
 & =\beta_{A}+\beta_{K}k_{t+1}+\mathrm{E}\left[\omega_{t+1}\mid\omega_{t},x_{t+1}=1\right]\\
 & =\beta_{K}k_{t+1}+g(\omega_{t},\underline{\omega}_{t+1})\end{aligned}$$
ここで, $g(\omega_{t},\underline{\omega}_{t+1})=\beta_{A}+\mathrm{E}\left[\omega_{t+1}\mid k_{t+1},x_{t+1}=1\right]$であり, 欠落バイアスを表している. $g$は観測できない$\omega_{t}$で構成されるが, \@ref(eq:exit-prob)から, $t+1$時点の存続確率$p_{t+1}$は$(k_{t+1},\omega_{t})$の関数で書けることが分かっている. さらに, 期首の\(k_{t+1}\)は前期の$k_{t},\mathit{inv}_{t}$に依存する(この$k_{t+1}$は$\omega_{t+1}$が決まる前の値なので, 経営者が投入量を決めるという話とは無関係). よって, $t+1$時点の$g(\cdot)$は$p_{t+1}$と$\omega_{t}$で表現できることがわかる(つまりこれは一種の**傾向スコア**である).

退出したかどうか($x_{t}$)はパネルデータの欠測で知ることができるため, $x_{t}$ に対してプロビット回帰[^probit-poly]をすることで確率の推定値$\hat{p}_{t}$を計算できる. さらに, $\omega_{t}$は, 第1段階で推定した$\phi_{t}(k_{t},\mathit{inv}_{t})=\beta_{A}+\beta_{K}k_{t}+\omega_{t}$から, 以下のように書ける.
$$\begin{aligned}
\hat{g}(\omega_{t},\underline{\omega}_{t+1})= & g(\hat{p}_{t+1},\hat{\phi}_{t}-\beta_{A}-\beta_{K}k_{t})\end{aligned}$$

$\hat{g}$と$\omega_{t}$のバイアスを$\hat{\xi}_{t+1}:=\omega_{t+1}-\hat{g}(\omega_{t},\omega_{t+1})$として, 当初の式\@ref(eq:op-model-reduced)に代入できる. **第2段階**[^2nd-stage]では$\hat{\beta}_{L}$, $\hat{\phi}_{t}$, $\hat{p}_{t}$を所与として, 以下のようなラグのある回帰式を推定する問題として扱える[^slide-abb]. 

$$\begin{aligned}
(y_{t+1}-\hat{\beta}_{L}l_{t+1})= & \beta_{K}k_{t+1}+g(\hat{p}_{t},\hat{\phi}_{t}-\beta_{A}-\beta_{K}k_{t})+\hat{\xi}_{t+1}+\varepsilon_{t+1}\end{aligned}(\#eq:OP-final)$$

(プロビットモデルの推定がバイアスなくできているならば)$\hat{\xi}_{t+1}$は平均ゼロでかつ$k_{t+1}$とも独立だが, $l_{t+1}$とは相関しているかもしれない. しかし$\beta_{L}$は第1段階で推定した$\hat{\beta}_{L}$を代入できるため, もはや気にする必要はない. $(\hat{\xi}_{t+1}+\varepsilon_{t+1})$を誤差項とみなすと, $\beta_{K}$は誤差項と相関しないので識別できる. そして$g(\cdot)$は構造がわからない非線形関数である. そこで, Olleyらは2通りの推定方法を試している. 1つは1段階目と同様に部分線形モデルとしてカーネル回帰を利用する方法, もう1つは, 多項式近似した上で非線形最小二乗法で解く方法である. つまり,
$$\begin{aligned}
y_{t+1}-\hat{\beta}_{L}l_{t+1}= & \beta_{K}k_{t+1}+\sum_{d=1}^{q}\sum_{m=0}^{d}\gamma_{m,d-m}\hat{h}_{t}^{m}\hat{p}_{t}^{d-m}+(\hat{\xi}_{t+1}+\varepsilon_{t+1})\end{aligned}$$
である. ここで, $\hat{h}_{t}:=\hat{\phi}_{t}-\beta_{A}-\beta_{K}k_{t}$で, $q$は多項式の次数である. なお, OP法では標準誤差を代数的に得られないため, **ブートストラップシミュレーション**で計算する必要がある. この方法で導出したモデルの統計的性質は @pakes1995Limit でより詳しく説明されている.

[^missing-bias]: 欠測が推定結果に及ぼす影響については, @Hoshino2009, @TakaiHoshinoNoma2016, @TakahashiWatanabe2017 のいずれかが参考になるだろう. ただしパネルデータでの問題については星野が少し触れている程度である. 英語でもいいならば @Wooldridge2010panel, Ch. 19 が解説している.
[^investment-monotone]: $k_{t}$に対して単調増加である理由はすでに示唆した. 一方で$\omega_{t}$が大きければ, 経営者はやはり将来の利益が大きくなると予測し, 設備投資を増やす傾向にある, という理由でこの仮定を説明できる. しかし現実のデータがこの仮定を満たすとは限らない. 実際には規模縮小のために資産を売却するという選択肢もあり, このとき設備投資はマイナス扱いになる. しかしこの仮定はかなり強い仮定であり, これ以降も @LevinsohnPetrin2003 などがこの仮定がもたらす問題が取り上げられている.
[^nonpara]: Olleyらは4次の多項式補間で計算しているが, これは次数を増やしてMSEおよび係数が変化しなくなるものを採用するというルールで推定したためである. 一方で, 年代で分割して試すと変化が見られたとしている. 多項式に特定する理由もないので, 部分線形モデルはカーネル平滑化による計算も可能だが, こちらは次元が増えると計算に時間がかかるという性質がある.
[^cf]: 特定のパラメータだけを推定するために, 操作変数の式を与えて内生性によるバイアスを排除する方法を**コントロール関数アプローチ**という. @CameronTrivedi2005, @Wooldridge2010panel にもコントロール関数の解説があるが, 日本語の教科書では見た記憶がない. ググってもコントロール関数のアイディアを解説しているのは明城 (2012, 製品差別化財の需要関数推定における内生問題について)くらいで, それも需要関数推定の問題に限定したものなので少しわかりにくいかもしれない. 解説したものは私が以前自分で書いたものくらいしかない (これもあまりおすすめできない). Kawaguchiが部分線形モデルでの応用に関して参考文献として挙げるToddとIchimuraの論文はHandbook of Econometricsに収録されているので研究機関に所属していない人間がアクセスするのは少し大変である. 代わりに @blundell2003Endogeneity ならば一般公開されている.
[^probit-poly]: Olleyらはこれも1段階目の推定と同様の方法で次数を決めた多項式展開でプロビット回帰をしている.
[^2nd-stage]: Olleyらの論文では$p_{t}$の推定を第2段階と呼んでいるので, これは原文では第3段階である. しかし後の解説では省略されることが多い.
[^slide-abb]: スライドでは余白節約のため\@ref(eq:OP-final)を $\hat{\omega}_{t}:=\hat{\phi}-\beta_{A}-\beta_{K}k_{t}$,$e_{t+1}:=\hat{\xi}_{t}+\varepsilon_{t+1}$と置き換えて表現している.

## モデルの事後診断

さて, 反事実分析でのモデルの評価は単に当てはまりが良いかだけでなく, 仮定と現実のデータが矛盾していないかの確認も同等かそれ以上に重要である (今回は研究のアイディアとRの実装を紹介するのが目的なのでこの工程は省く).

ここでもOlleyらの研究を例にする. まず, 彼らの仮定が正しければ, 単なる最小二乗法や, IVなどの方法では結果にバイアスが発生するはずである.
よって, 提案する推定方法との比較が必要である. あるいは, サンプルセレクションバイアスが実際に発生しているか確認するため, バランスドパネルでの結果と比較することも必要である.
さらに, 市場の構造は法規制の変化によって変わりうる. 法改正のタイミングで年代ごとに区切って当てはまりを確認する必要がある.

さらに, モデルのロバストネス分析として, モデルの仮定のうち特に重要な, 投資関数の式\@ref(eq:investment)を検討している.
投資が資本と観察できない生産性によって決まるという仮定が正しくない場合, $\beta_{L}$の推定にバイアスが生じる. これを検証するには最後の式\@ref(eq:OP-final)の説明変数に,
現在の$l_{t}$を加えることでできる (このようなタイプの検証は**特定化のテスト**と呼ばれる).

## OP法以降の研究

@LevinsohnPetrin2003 はOP法を改良した方法を提案している. 多くの企業の会計情報を見ると, 実際には設備投資は必ずしも発生せず, ゼロが多い. これは投資関数の単調性と矛盾するため, 中間投入財 (材料費) の情報を利用することを提案している. 現在はさらに @ackerberg2015Identification が改良版を提案して, これがスタンダードになりつつあるらしいが今回は省略.


# R での実装

## データのとり方

企業の生産関数推定で一番むずかしいのは実はここかもしれない. というのも, 理論上の$K$や$L$はどこにも記載されていないからだ.
これはけっこう地道な計算が必要になる. 例えば企業の決算報告書などから地道に計算する, あるいは日銀やDBJのような政策金融機関が整備しているデータベースをなんらかの手段で見せてもらう,
などである. あるいは, 農業や工業など資源と生産物の関係が分かりやすい特定の産業だけでを分析の対象とすることもできる. 例えば農業で,
$K$を乳牛の頭数, 原材料を飼料, $Y$をミルクの生産量とした分析も可能だろう (昔見た気がするがすぐには見つけられなかった). 今回はとてもそんなことをしている暇も金もコネもないので[^real-data], 後述する乱数データを使う.

`estprod`パッケージが, Olley-PakesやLevinsohn-Petrinの方法を実装したという. さらに, このパッケージには練習用の乱数データセットも用意されている.
この乱数がどのような方法で生成されたかがわからないので, 実質的に動作確認にしかならないが, まずはこれを使ってみた. しかし, 1段階目の推定を検証してみたところ, 残差プロットの形がかなり変になった. 残差プロットが必ず完璧な45度線を引かなけばならない, ということはないが, データの生成方法は分からないこともあって用例として不安である[^estprod]. よって, @kawaguchi2019ECON の課題2に沿って**疑似データを生成してやり直す**ことにした.

しかし, 去年の講義内容とはいえ課題の答案を勝手に一般公開してしまうのも無配慮すぎる気がするため, **乱数データを作成するプログラムは非公開**として, 当初の課題から乱数シード値などを変更して作成したデータ`df_sample`, `df_ground_truth`のみ公開する. 前者は標本データで, 後者は$\omega_{t},p_{t}$など本来は観測できない変数も見えるようにしたデータである(表\@ref(tab:var-table), 以下, 後者を「正解データ」と呼ぶ). 以降はこの`df_sample`を使ってパラメータを推定する. 推定したい3つのパラメータは元の課題どおり,
$\beta_{A}=1,\beta_{K}=0.7,\beta_{L}=0.2$としている.

Table: (\#tab:var-table) 乱数データの変数対応表

|                   | `i`  |  `t`  |    `k`    |   `k_lag`   |        `inv`         |       `inv_lag`        |    `x`    |    `y`    |     `y_true`      |    `omega`     |   `p_x`   |
  | :--: | :--: | :---: | :---: | :----: | :------: | :-----: | :----: | :-------: | :------: | :------: | :-------: |
  | 標本 | 企業ID | $t$ | $k_{t}$ | $k_{t-1}$ | $\mathit{inv}_{t}$ | $\mathit{inv}_{t-1}$ | $x_{t}$ | $y_{t}$ |        NA         |       NA       |    NA     |
  | 正解データ | 企業ID | $t$ | $k_{t}$ | $k_{t-1}$ | $\mathit{inv}_{t}$ | $\mathit{inv}_{t-1}$ | $x_{t}$ | $y_{t}$ | 退出しない場合の$y_{t}$ | $\omega_{t}$ | $p_{t}$ |

なお, 乱数生成したため, 偶然にも一部の企業が全期間で退出している. データとして不自然だが計算には問題ないので修正せずそのままにしている.

```{r load-dataset}
df_sample <- read_csv("data/df_sample.csv", col_types = cols(i = col_factor(), x = col_integer()))
df_ground_truth <- read_csv("data/df_ground_truth.csv", col_types = cols(i = col_factor(), x = col_integer()))
```

[^real-data]: いくつかの計量経済学の教科書の著者は練習問題用にデータセットを公開している. 例えば [Wooldridge data sets](http://fmwww.bc.edu/ec-p/data/wooldridge/datasets.list.html), [Datasets for empirical exercises in Econometrics](http://fhayashi.fc2web.com/datasets.htm) など. しかし, 今回の問題に適した企業別パネルデータは見つけられなかった.
[^estprod]: このパッケージはCRANに登録されているものの, 作者のプロフィールがよくわからない. また, [githubリポジトリ](https://github.com/cran/estprod)にもほとんど関心が集まっていないため, 動作がどの程度適切なのかあまり確認が進んでいなさそうだ. あと, `coefs()`, `predict()`, といった`stats`の基本関数にも対応していない. もちろん`stargazer`に対応していないし, `estimatr`にも収録されていない. 他にも, 企業退出を考慮しているように見えて実は不均衡パネルデータを想定した実装になっていないなどの問題があった.

## $\beta_{L},\phi$の識別

1段階目の推定は部分線形カーネル回帰で推定する. これは`np`パッケージで提供されている. 今回は`npplregbw()`でデータから最適バンド幅を決定し, 決定したバンド幅を`npplreg()`に与えて推定する[^1st-pooling][^np-na-action]. この推定結果を残差プロット等で確認したら, $p_{t}$の推定を行う. 1段階目のモデル\@ref(eq:OP-1st)は, 誤差項と相関のある部分線形モデルであり, 単に多項式やスプライン近似を適用しただけでは正しく$\phi$を推定できないため, `npplregbw(formula = y ~ l k + inv)`としても誤差項の相関のために$\beta_{L}$は正しく推定されず, 全く異なる値になる. そこで部分線形モデルとコントロール関数アプローチの併用によって$\beta_{L},\phi$を識別する. このためには, `formula = y ~ l + k | k + inv` と指定する. これで, $\beta_{L},\phi$を識別できるが, もちろんこの時点では $\beta_{K}$は正しく推定できていない.

```{r np-npplreg, eval=run_heavy, echo=T}
 # somehow `na.action` doesn't work so we need `drop_na()`
bw_1st <- npplregbw(formula = y ~ l | k + inv, data = df_sample %>% drop_na(y, k, l, inv))
fit_1st_np <- npplreg(bw_1st)
summary(fit_1st_np)
```

なお, `np`パッケージのノンパラメトリック回帰は計算負荷がけっこうある. そこで, 面倒だったらOlleyらと同様に以下のような多項式近似でごまかしても良い.

```{r 1st-poly, eval=F, echo=T}
lm(y ~ l + poly(k, inv, degree = q), data = df_sample)
```

`degree = q` は多項式の次数で, `formula`を評価する際には$\sum_{r\leq q}k_{t}^{r}\mathit{inv}_{t}^{q-r}$ と認識される. 何も考えずに `degree = 4` としてもそこそこそれらしい値を推定できるが[^sim-simplicity], Olleyのやるように推定値とMSEが安定する次数を選ぶのがスマートだろう. しかし残念ながら `stats::step()` 関数には `poly()` の次数を調整する機能がないので, ループ処理か予め決めた次数までを並列処理して比較する必要がある. そこで, 以下のように10次までの多項式あてはめを一度に実行する処理を書いた[^poly-redundant].

```{r 1st-poly-loop, echo=T}
set.seed(42)
df_fit_1st_poly <- tibble(q= 1:10) %>%
  mutate(model = map(q, function(q){
    if(q == 1){
      poly_term <- "+ k"
    } else{
      poly_term <-  paste("+ poly(k, inv, degree =", q, ")")
    }
    lm(as.formula(paste("y ~ 1 + l + k", poly_term )), data = df_sample)
  })) %>%
  mutate(
    rmse = map_dbl(model, function(x) sqrt(mean(x$residuals^2))),
    coef = map(model, function(x) enframe(coef(x)) %>% filter(name %in% c("k", "l")))
  ) %>% unnest(coef) %>% pivot_wider(names_from = name, values_from = value)
# we select 5th order, the estimate of beta_l = .161
fit_1st_poly <- df_fit_1st_poly$model[[5]]
```

ところで, 現在はノンパラメトリックな回帰モデルとして**スプライン回帰**の研究が進んでいる(基本的な事項は @HstieTibshrianiFriedman2009, Ch. 5), @SakamotoEtAl2009 などを参照). スプライン回帰にもバリエーションがあるが, 典型的なものは区間ごとに異なる次数の多項式で表現するため,
単純な多項式近似よりも表現の自由度の高い非線形近似の方法である. さらに`SemiPar::spm()`は部分線形モデルに対しても罰則付き最尤推定という方法で当てはめたスプライン回帰を計算することができる. 表現の自由度で言えばカーネル回帰のほうがさらに優れているが, 計算時間はこちらのほうが少ない. 多項式近似である程度うまくいくのだからスプライン回帰でもうまくいくと思ったのだが, なぜかうまく行かない. 原因を調べるのがめんどくさくなったので読者の課題とする.

```{r spm-trial, eval=F, echo = T}
tmp <- select(df_sample, y, k, l, inv)
f_spm <- spm(form = tmp$y ~ tmp$l + f(tmp$k, tmp$inv), omit.missing = T)
summary(f_spm)
```

[^1st-pooling]: Olleyら本来のやり方を踏襲するなら, 時点ごとに分けて当てはめるのが正統だが, 今回はプール推定する.
[^np-na-action]: なお, `np`パッケージに与えるデータを`tibble`で与えるとエラーが出る(しかも一見関係ないメッセージ)ので注意する. また, デフォルト設定である `na.action = stats::na.omit` で欠損値を含むレコードを除外して計算するはずだが, なぜか機能しない. そのため入力データから欠損値を除外する処理を自分で書かなければならない.
[^sim-simplicity]: 今回これでもうまくいくのは, データの生成過程が単純だからである. 実際のデータに応用するときは, データの特性, つまりどういう現象から取得したデータなのかをよく考えて方法を柔軟に変えねばならない.
[^poly-redundant]: `estimatr`や`tidymodels`などならもっと簡単に書けないかとも考えたが, これらのパッケージは今回使う推定方法に意外と対応していない.

## 存続確率の推定

Olleyは, 理論モデルの仮定から$p_{t+1}$を決定する関数は$k_{t},\mathit{inv}_{t}$の2変数関数で表現できることから, この2変数を使ったプロビットモデル(ロジットリンク関数を標準正規分布に置き換えたもの)と多項式近似またはカーネル回帰を組み合わせて確率を推定している.

今回は正解が分かっているため, $p_{t+1}$と$k_{t},\mathit{inv}_{t}$の関係を知ることができる. 2変数の散布図とカーネル回帰で近似曲線を当てはめたものが以下のようになる. プロビットモデルならば, $p_{t+1}$を標準正規分布の逆分布関数 (分位点関数)で変換した値と説明変数が線形相関していれば, その説明変数を使うことで当てはまりの良いモデルができると予想できる. しかし, 図\@ref(fig:prob-corr)からわかるように, 2つの説明変数との相関関係はいずれもあまり直線的でないことがわかる.

```{r prob-corr, fig.cap="真の確率と説明変数の相関"}
g1 <- ggplot(df_ground_truth, aes(x = k_lag, y = boot::logit(p_x))) + geom_point() + stat_smooth() +
  thm + theme(axis.title.y = element_text(angle = -90)) + labs(x = TeX("$k_t$"), y = TeX("\\mathit{logit}(p_{t+1})"))
g2 <- ggplot(df_ground_truth, aes(x = inv_lag, y = boot::logit(p_x))) + geom_point() + stat_smooth() +
  thm + theme(axis.title.y = element_text(angle = -90)) + labs(x = TeX("$\\mathit{inv}_t$"), y = TeX("\\mathit{logit}(p_{t+1})"))
g1 | g2
```

よって, 単なる一般化線形回帰ではなく, より複雑な非線形近似方法を使う必要があるとわかる. Rでは多項式プロビット回帰は`glm()`と`poly()`の組み合わせで,
例えば以下のようにして計算できる. ここでも次数をデータの当てはまりで決定するため$\beta_L$の推定と同様に次数を変えて並列処理している.

```{r, eval=F}
glm(x~poly(k_lag, inv_lag, q), family = binomial(link = "probit"), data = df_sample)
```

```{r estimate-prob}
std_Brier <- function(actual, prediction){
  sqrt(mse(actual, prediction) / mse(actual, mean(actual)))
}
set.seed(42)
df_fit_survival <-tibble(q = 1:10) %>%
  mutate(model = map(q, function(q){
    if(q == 1){
      poly_term <- "k_lag + inv_lag"
    } else{
      poly_term <-  paste("poly(k_lag, inv_lag, degree =", q, ")")
    }
    glm(as.formula(paste("x ~", poly_term)),
        data = df_sample %>% drop_na(x, k_lag, inv_lag),
        family = binomial(link = "probit"), na.action = na.omit)
  }))
df_fit_survival <- df_fit_survival %>%
  mutate(
    actual = map(1:n(), function(x) drop_na(df_sample, x, k_lag, inv_lag)$x),
    pred_p = map(model, ~fitted(.x)),
    pred_label = map(pred_p, ~.x > .5),
    rate = map_dbl(actual, mean),
    acc = map2_dbl(actual, pred_label, accuracy),
    precision = map2_dbl(actual, pred_label, precision),
    recall = map2_dbl(actual, pred_label, recall),
    f1 = map2_dbl(actual, pred_label, f1),
    log_loss = map2_dbl(actual, pred_p, logLoss),
    neg_RIG = log_loss / (-rate * log(rate) - (1 - rate) * log(1 - rate)) - 1,
    std_Brier = map2_dbl(actual, pred_p, ~std_Brier(.x, .y)),
    cor = map2_dbl(actual, pred_p, ~cor(.x, .y)),
    unique_rate = map_dbl(pred_p, ~n_distinct(.x)/length(.x))
  ) %>% select(-actual, -pred_p, -pred_label)
fit_survival <- df_fit_survival$model[[2]] # we select 2nd order.
```

1段階目で使用した`np`パッケージは線形回帰のみ対応しているため, プロビットモデルの推定はできない. 他のRのパッケージでは,
例えば各変数をスプライン補間などで非線形変換した**一般化加法的モデル** (GAM[^gam-materials])を計算できる`mgcv`パッケージ, 一般化ガウシアンカーネル回帰のできる`bkmr`パッケージや, **一般化部分線形モデル**を計算できる`gplm`などがある. これらはいずれもノンパラメトリックあるいはセミパラメトリックなモデルと呼ばれるが, それぞれ表現方法が違うため, 当てはまりの良さも変わってくる[^nonlinear-nonpara]. また, このパートではそもそも係数の推定が不要なので, 確率さえ推定できているならランダムフォレストのような自由度の高いモデルでも問題ないだろう[^prob-estimate]. 今回は`mgcv`, `bkmr`, `gplm`, `ranger`などを使って推定してみた.

なおKawaguchiの課題では退出行動を省略しているのでこのパートは存在しない.

[^gam-materials]: 参考: @HstieTibshrianiFriedman2009, Ch. 9), [鈴木大慈の講義スライド](http://ibis.t.u-tokyo.ac.jp/suzuki/lecture/2015/dataanalysis/L12.pdf), あるいは言語研究者からの観点だが『[GLMM/GA(M)Mの文献案内](http://www.akira-murakami.com/?p=320)』で挙げられているリストも興味深い. また, [以前紹介](http://ill-identified.hatenablog.com/entry/2018/05/28/020224)したprophetのモデルも一般化加法的モデルの一種である.
[^nonlinear-nonpara]: この場でこういったノンパラメトリック回帰の方法まで細かく説明するのは面倒なのでしない. GAMやカーネル回帰はとりあえず @HstieTibshrianiFriedman2009, ch. 5 の教科書を読んで欲しい. または @li2007Nonparametric の教科書か. それ以外の細かい話は各パッケージのマニュアルで参照されている論文を読んでほしい.
[^prob-estimate]: ただし, ここで重要なのは退出・存続の分類ではなく確率の推定であることに注意する: 結果変数$x_{t}$の条件分布ではなく$p_{t}$の分布を知る必要があるため,
    関数形の特定に依存する問題である.

## \(\beta_{A},\beta_{K}\)の推定

最後の推定は, $\omega_{t}$を推定し, さらに企業の退出行動によるサンプルセレクションバイアスを補正するための計算である.

まず, 計算の無駄をなくすためにの関数のように推定に必要なラグ項をデータフレーム側で作成しておく. `mutate()`関数内で`dplyr::lag()`を使えばそれぞれのラグ項を得られる[^conflicted-lag]. 元のパネルデータを`df_sample`, 1段階目の推定結果を`fit_1st`として, 2段階目の推定のためのデータセット`df_sample_2nd`を作成している.
`y_bl`, `phi`, `p_x` はそれぞれ$y_{t}-\hat{\beta}_{L}l_{t},\hat{\phi}_{t},p_{t}$に対応する.

```{r define-2nd-functions-displayed, echo=T}
make_2nd_sample <- function(data, beta_l, phi, p_x){
  data %>%
    mutate(
      bl = l * beta_l * l,
      y_bl = y - bl,
      phi = phi,
      p_x = p_x
    ) %>%
    arrange(i, t) %>% group_by(i) %>% mutate(
      phi_lag = lag(phi),
    ) %>% ungroup
}
```
```{r define-2nd-functions-hidden}
get_beta_a <- function(data, beta_k, beta_l){
  mean(with(data, y - beta_k * k -beta_l * l), na.rm = T)
}
get_fitted <- function(data, beta_k, beta_l, beta_a = NULL){
  if(is.null(beta_a)){
    beta_a <- get_beta_a(data, beta_k, beta_l)
  }
  with(data, beta_a + beta_k * k + beta_l * l)
}

plot_iterated_convergence <- function(data, max_degree = 6, size = 20){
  # plot iterated results
  data %>% filter(d <= max_degree, n <= size) %>% mutate(
    beta_a_start = map_dbl(start, ~.x[1]),
    beta_k_start = map_dbl(start, ~.x[2])
  ) %>% ggplot(aes(x = beta_a_start, y = beta_k_start, xend = beta_a, yend = beta_k, color = log(rmse))) +
    geom_segment(arrow = arrow(length = unit(.05, "npc")), size = .5) + geom_point() +
    facet_wrap(~d) + thm + theme(legend.position = "right") +
    labs(x = TeX("$\\beta_A$"), y = TeX("$\\beta_K$"))
}
plot_iterated_errorbar <- function(data){
  # plot summarised result
  skim(data %>% group_by(d)) %>% select(d, skim_variable, numeric.mean, numeric.sd) %>%
    filter(skim_variable %in% c("beta_a", "beta_k")) %>%
    mutate(variable = factor(skim_variable, labels = c(TeX("$\\beta_A$"), TeX("$\\beta_K$")))) %>%
    ggplot(aes(x = d, y =numeric.mean)) + geom_line() +
    geom_errorbar(aes(ymin = numeric.mean - numeric.sd, ymax = numeric.mean + numeric.sd)) +
    facet_wrap(~variable, scales = "free_y", labeller = label_parsed) +
    scale_x_continuous(breaks = 1:8) + thm + theme(axis.title.y = element_text(angle = 90)) + labs(x = TeX("$q$"), y = "estimates")
}


# make 2nd-stage dataset
phi_hat <- predict(fit_1st_np, newdata = df_sample %>% select(y, k, l, inv)) %>% as.numeric - df_sample$l * coef(fit_1st_np)["l"]
df_sample_2nd_np <- make_2nd_sample(
  df_sample, coef(fit_1st_np)["l"], phi_hat,
  predict(fit_survival, type = "response", newdata = df_sample)
)

df_sample_2nd_poly <- make_2nd_sample(
  df_sample, coef(fit_1st_poly)["l"],
  predict(fit_1st_poly, newdata = df_sample) - coef(fit_1st_poly)["l"] * df_sample$l,
  predict(fit_survival, type = "response", newdata = df_sample))

df_ground_truth_2nd <- make_2nd_sample(
  data = df_ground_truth, beta_l = beta_l.,
  phi =  beta_a. + beta_k. * df_ground_truth$k + df_ground_truth$omega,
  p_x = df_ground_truth$p_x
)
```

ここで一旦, 実際には観測できない $\beta_{L},\omega_{t}$の情報を使って$\beta_{K}$を正しく推定できることを確認してみる. 以下のように$y_{t}-\beta_{L}l_{t}-\omega_{t}$に対して線形回帰をした結果, $\beta_{A}=1.00,\beta_{K}=0.70$とほぼ正確な値が出ている.

```{r, echo=T, eval=F}
lm(formula = y_bl - omega ~ k, data = df_ground_truth_2nd) %>% summary()
```

```{r, lm-cheating}
lm(y_bl - omega ~ k, data = df_ground_truth_2nd) %>%
  summary() %>% xtable::xtable() %>% kable(digits = 3)
```

しかし, 実際には$\omega_{t}$は不明であり, 1段階目で推定した$h_{t}$と確率$p_{t}$から推定する[^missing-model]. Olleyらの方法と同様に多項式近似による非線形最小二乗法で計算する.

なお再び正解データを覗き見すると, $\hat{\phi}_{t},\hat{p}_{t+1}$と$\omega_{t+1}$の関係はそれぞれ図\@ref(fig:omega-corr)のようになっている.

```{r omega-corr, fig.cap="$\\hat{\\phi}_{t},\\hat{p}_{t+1}$と$\\omega_{t+1}$の正解データとの相関関係"}
g1 <- df_sample_2nd_poly %>% left_join(select(df_ground_truth, i, t, omega)) %>%
  ggplot(aes(x = phi_lag, y = omega)) + geom_point() + stat_smooth() +
  labs(x = TeX("$\\hat{\\varphi}_{t}$"), y = TeX("$\\omega_{t+1}$")) + thm
g2 <- df_sample_2nd_poly %>% left_join(select(df_ground_truth, i, t, omega)) %>%
  ggplot(aes(x = p_x, y = lag(omega))) + geom_point() + stat_smooth() +
  labs(x = TeX("$\\hat{p}_{t+1}$"), y = TeX("$\\omega_{t+1}$")) + thm
g1 | g2
```

非線形最小二乗法の計算は`stats::nls()`関数が提供されており, この関数は`formula`で記述できたり, `lm()`のように標準誤差を計算してくれたりする. しかし今回は多項式部分を`formula`で記述するのがやや面倒なのと, `nls()`の計算する標準誤差が意味をなさないため, `optim()`関数で計算する. 目的関数は以下のように, 任意の次数の\@ref(eq:OP-final)のMSEを計算できるような関数を作った.

```{r obj-op-2nd}
obj_op_2nd <- function(param, data, degree){
  # param = [beta_a, beta_k, gamma1, gamma2, ...]
  h <- with(data, phi_lag - exp(param[2]) * k_lag)
  poly <- map_dfc(
    1:degree,
    function(d) {reduce(0:d, ~.x + param[2 + sum(1:d + 1) - d - 1 + 1 + .y] * with(data, h^(d - .y) * p_x^(.y)), .init = 0)}
  ) %>% rowSums
  mean((with(data, y_bl - param[1] - exp(param[2]) * k) - poly)^2, na.rm = T)
}
```

$p_{t},h_{t}$の交差項を含む$q$次多項式の第$d$次項は$d+1$個存在するため, 1次から$d-1$次までの項の総数は$\sum_{i=1}^{d}i$, 項の総数は$\sum_{d=1}^{q}(d+1)$個であるから, 上記のように`purrr::map_dfc()`, `purrr::reduce()`を入れ子にして計算した[^computing]. なお, 数値計算上の観点から初期値にも注意する必要がある. これは補遺\@ref(init)で詳しく説明している. また, Olleyらはさらにここでもカーネル回帰も試しているが, パラメータが含まれる非線形式のカーネル回帰は自分で1から書かなければならず面倒になったので今回は省略する.

```{r, estimate-2nd-stage, eval=run_heavy}
set.seed(42)
df_fit_2nd_np <- expand_grid(
  d = 1:8, n = 1:100
) %>% mutate(
  start = map(d, ~c(rnorm(n = 1, sd = .5),
                    log(1 - unname(coef(fit_1st_np)["l"])) + rnorm(n = 1, sd = .3),
                    rnorm(n = sum(1:.x + 1), sd = .5)))
) %>% mutate(
  model = map2(start, d, ~optim(par = .x, fn = obj_op_2nd, data = df_sample_2nd_np, degree = .y))
) %>% mutate(
  beta_a = map_dbl(model, ~.x$par[1]),
  beta_k = map_dbl(model, ~exp(.x$par[2])),
  rmse = map_dbl(model, ~sqrt(.x$value))
)
fit_2nd_np <- filter(df_fit_2nd_np, d == 2)$model[[1]]

# polynomial version
set.seed(42)
df_fit_2nd_poly <- expand_grid(
  d = 1:8, n = 1:100
) %>% mutate(
  start = map(d, ~c(rnorm(n = 1, sd = .5),
                    log(1 - unname(coef(fit_1st_poly)["l"])) + rnorm(n = 1, sd = .3),
                    rnorm(n = sum(1:.x + 1), sd = .5)))
) %>% mutate(
  model = map2(start, d, ~optim(par = .x, fn = obj_op_2nd, data = df_sample_2nd_poly, degree = .y))
) %>% mutate(
  beta_a = map_dbl(model, ~.x$par[1]),
  beta_k = map_dbl(model, ~exp(.x$par[2])),
  rmse = map_dbl(model, ~sqrt(.x$value))
)
fit_2nd_poly <- filter(df_fit_2nd_poly, d == 2)$model[[1]]
```
```{r estimate-2nd-gam-rf, eval=run_heavy}
# addendum: estimate by GAM or random forest
fit_gam <- mgcv::gam(x ~ s(k_lag) + s(inv_lag), family = binomial(link = "probit"), data = df_sample %>% drop_na(x, k_lag, inv_lag))
fit_rf <- ranger::ranger(formula = x ~ k_lag + inv_lag, data = drop_na(df_sample, x, inv_lag, k_lag))

# GAM
df_sample_2nd_gam <- make_2nd_sample(
  df_sample, coef(fit_1st_poly)["l"],
  predict(fit_1st_poly, newdata = df_sample) - coef(fit_1st_poly)["l"] * df_sample$l,
  predict(fit_gam, type = "response", newdata = df_sample))
df_fit_2nd_gam <- expand_grid(
  d = 1:8, n = 1:100
) %>% mutate(
  start = map(d, ~c(rnorm(n = 1, sd = .5), log(1 - coef(fit_1st_poly)["l"]) + rnorm(n = 1, sd = .3), rnorm(n = sum(1:.x + 1), sd = .5)))
) %>% mutate(
  model = map2(start, d, ~optim(par = .x, fn = obj_op_2nd, data = df_sample_2nd_gam, degree = .y))
) %>% mutate(
  beta_a = map_dbl(model, ~.x$par[1]),
  beta_k = map_dbl(model, ~exp(.x$par[2])),
  rmse = map_dbl(model, ~sqrt(.x$value))
)
fit_2nd_gam <- filter(df_fit_2nd_gam, d == 2)$model[[1]]

# random forest
p_rf <- drop_na(df_sample, x, inv_lag, k_lag) %>% select(i, t) %>%
  mutate(p = predict(fit_rf, data = drop_na(df_sample, x, k_lag, inv_lag))$prediction) %>%
  left_join(df_sample, ., by = c("i", "t")) # somewhat inflexible...
df_sample_2nd_rf <- make_2nd_sample(
  df_sample, coef(fit_1st_poly)["l"],
  predict(fit_1st_poly, newdata = df_sample) - coef(fit_1st_poly)["l"] * df_sample$l,
  p_rf$p)
df_fit_2nd_rf <- expand_grid(
  d = 1:8, n = 1:100
) %>% mutate(
  start = map(d, ~c(rnorm(n = 1, sd = .5), log(1 - coef(fit_1st_poly)["l"]) + rnorm(n = 1, sd = .3), rnorm(n = sum(1:.x + 1), sd = .5)))
) %>% mutate(
  model = map2(start, d, ~optim(par = .x, fn = obj_op_2nd, data = df_sample_2nd_rf, degree = .y))
) %>% mutate(
  beta_a = map_dbl(model, ~.x$par[1]),
  beta_k = map_dbl(model, ~exp(.x$par[2])),
  rmse = map_dbl(model, ~sqrt(.x$value))
)
fit_2nd_rf <- filter(df_fit_2nd_rf, d == 2)$model[[2]]
```

そして標準誤差を代わりに (ノンパラメトリック) **ブートストラップ法**で計算する. ブートストラップ法による標準誤差の計算方法は補遺\@ref(boot)で補足説明している.

```{r compute-bootstrap, eval=run_heavy}
estimate_op_poly <- function(data, degree_1, degree_p, degree_2nd){
  fit_1st <- lm(y ~ l + poly(k, inv, degree = degree_1), data = data, na.action = na.omit)
  beta_l <- unname(coef(fit_1st)["l"])
  rmse_1st <- sqrt(mean(resid(fit_1st)^2))
  fit_exit <- glm(x ~ l + poly(k_lag, inv_lag, degree = degree_p), data = data %>% drop_na(k_lag, inv_lag), family = binomial(link = "probit"))
  std_Brier <- std_Brier(fit_exit$data$x, fitted(fit_exit))
  df_2nd <- make_2nd_sample(
    data, beta_l,
    phi = predict(fit_1st, newdata = data) - beta_l * data$l,
    p_x = predict(fit_exit, newdata = data, type = "response")
  )
  fit_2nd <- optim(
    par = c(rnorm(n = 1, sd = .5), log(ifelse(1 - beta_l <=0, .5, 1 -beta_l)) + rnorm(n = 1, sd = .3), rnorm(n = sum(1:degree_2nd + 1), sd = .5)),
    fn = obj_op_2nd, data = df_2nd, degree = degree_2nd)
  beta_a <- mean(with(df_2nd, y_bl - exp(fit_2nd$par[2]) * k), na.rm = T)
  return(c(beta_a = beta_a, beta_l = beta_l, beta_k = unname(exp(fit_2nd$par[2])), rmse_1st = rmse_1st, std_Brier = std_Brier, rmse_2nd = sqrt(fit_2nd$value)))
}
estimate_op_gam <- function(data, degree_1, degree_2nd){
  fit_1st <- lm(y ~ l + poly(k, inv, degree = degree_1), data = data, na.action = na.omit)
  beta_l <- unname(coef(fit_1st)["l"])
  rmse_1st <- sqrt(mean(resid(fit_1st)^2))
  fit_exit_gam <- mgcv::gam(x ~ s(k_lag) + s(inv_lag), family = binomial(link = "probit"), data = data)
  std_Brier <- std_Brier(fit_exit_gam$y, fitted(fit_exit_gam))
  df_2nd <- make_2nd_sample(
    data, beta_l,
    phi = predict(fit_1st, newdata = data) - beta_l * data$l,
    p_x = predict(fit_exit_gam, newdata = data, type = "response")
  )
  fit_2nd <- optim(
    par = c(rnorm(n = 1, sd = .5), log(ifelse(1 - beta_l <=0, .5, 1 -beta_l)) + rnorm(n = 1, sd = .3), rnorm(n = sum(1:degree_2nd + 1), sd = .5)),
    fn = obj_op_2nd, data = df_2nd, degree = degree_2nd)
  beta_a <- mean(with(df_2nd, y_bl - exp(fit_2nd$par[2]) * k), na.rm = T)
  return(c(beta_a = beta_a, beta_l = beta_l, beta_k = unname(exp(fit_2nd$par[2])), rmse_1st = rmse_1st, std_Brier = std_Brier, rmse_2nd = sqrt(fit_2nd$value)))
}
estimate_op_rf <- function(data, degree_1, degree_2nd, ...){
  fit_1st <- lm(y ~ l + poly(k, inv, degree = degree_1), data = data, na.action = na.omit)
  beta_l <- unname(coef(fit_1st)["l"])
  rmse_1st <- sqrt(mean(resid(fit_1st)^2))
  fit_exit_rf <- ranger::ranger(formula = x ~ k_lag + inv_lag, data = drop_na(df_sample, x, inv_lag, k_lag))
  p <- drop_na(data, x, inv_lag, k_lag) %>% select(i, t) %>%
    mutate(p = predict(fit_rf, data = drop_na(data, x, k_lag, inv_lag))$prediction) %>%
    left_join(data, ., by = c("i", "t"))
  p <- p$p
  std_Brier <- std_Brier(drop_na(df_sample, x, k_lag, inv_lag)$x, fit_rf$predictions)
  df_2nd <- make_2nd_sample(
    data, beta_l,
    phi = predict(fit_1st, newdata = data) - beta_l * data$l,
    p_x = p
  )
  fit_2nd <- optim(
    par = c(rnorm(n = 1, sd = .5), log(ifelse(1 - beta_l <=0, .5, 1 -beta_l)) + rnorm(n = 1, sd = .3), rnorm(n = sum(1:degree_2nd + 1), sd = .5)),
    fn = obj_op_2nd, data = df_2nd, degree = degree_2nd)
  beta_a <- mean(with(df_2nd, y_bl - exp(fit_2nd$par[2]) * k), na.rm = T)
  return(c(beta_a = beta_a, beta_l = beta_l, beta_k = unname(exp(fit_2nd$par[2])), rmse_1st = rmse_1st, std_Brier = std_Brier, rmse_2nd = sqrt(fit_2nd$value)))
}
get_stat_boot <- function(data, indices, indices_, estimate, params){
  # estimate = estimate function
  # params = listed model hyper parameters (e.g., degrees of polynomials)
  data$y <- with(data, fitted + resid[indices])
  do.call(estimate, args = c(list(data = data), params))
}

op_params_np <- list(beta_k = exp(fit_2nd_np$par[2]), beta_l = unname(coef(fit_1st_np)))
op_params_poly <- list(beta_k = exp(fit_2nd_poly$par[2]), beta_l = unname(coef(fit_1st_poly)["l"]))
op_params_gam <- list(beta_k = unname(exp(fit_2nd_gam$par[2])), beta_l = unname(coef(fit_1st_poly)["l"]))
op_params_rf <- list(beta_k = unname(exp(fit_2nd_rf$par[2])), beta_l = unname(coef(fit_1st_np)["l"]))

set.seed(42)
boot_poly <- df_sample_2nd_poly %>%
  mutate(., fitted = if_else(is.na(y), NA_real_, do.call(get_fitted, args = c(list(data = .), op_params_poly))),
         resid = y - fitted) %>%
  boot(., get_stat_boot, R = 100, strata = .$x, estimate = estimate_op_poly, params = list(degree_1 = 3, degree_p = 2, degree_2nd = 2))
set.seed(42)
boot_gam <- df_sample_2nd_gam %>%
  mutate(., fitted = if_else(is.na(y), NA_real_, do.call(get_fitted, args = c(list(data = .), op_params_gam))),
         resid = y - fitted) %>%
  boot(., get_stat_boot, R = 100, strata = .$x, estimate = estimate_op_gam, params = list(degree_1 = 3, degree_2nd = 2))
set.seed(42)
boot_rf <- df_sample_2nd_rf %>%
  mutate(., fitted = if_else(is.na(y), NA_real_, do.call(get_fitted, args = c(list(data = .), op_params_rf))),
         resid = y - fitted) %>%
  boot(., get_stat_boot, R = 100, strata = .$x, estimate = estimate_op_rf, params = list(degree_1 = 3, degree_2nd = 2))


plot_boot_hist <- function(boot, name_params = c("beta_a", "beta_l", "beta_k", "RMSE_1st", "std_Brier", "RMSE_2nd")){
  as_tibble(boot$t) %>% set_names(name_params) %>%
    pivot_longer(cols = everything()) %>% filter(str_detect(name, "^beta")) %>%
    mutate(name = factor(name, labels = c(TeX("$\\beta_A$"), TeX("$\\beta_K$"), TeX("$\\beta_L$")))) %>%
    ggplot(aes(x = value)) + geom_histogram(bins = as.integer(max(boot$R/10), 10)) +
    facet_wrap(~name, scales="free", ncol = 1, labeller = label_parsed)
}
```
```{r op-final-result}
df_result_table_op <- list(OP_poly = boot_poly, OP_GAM = boot_gam, OP_RF = boot_rf) %>%
  map2_dfr(., names(.), ~skim(.x$t) %>% select(skim_variable, numeric.mean, numeric.sd) %>%
             mutate(name = names(.x$t0), model = .y) %>%
             rename(value = numeric.mean, std_err = numeric.sd) %>% select(name, value, std_err, model))
```

以上で説明した内容は, Githubリポジトリの`all_public.R`にまとめてある.

[^missing-model]: 欠測メカニズムが分かっている場合, セレクションバイアスを補正する**Heckmanの選択モデル** (Heckit, or Type-II Tobit)モデルが応用できる場合がある. しかし今回は欠測確率$p_{t}$が$k_{t}$に影響して決まるため, Heckitは意味をなさない.
[^conflicted-lag]: `plm`と`stats`にも`lag()`関数があるが, こちらは`mutate()`内で使用しても評価されないし, エラーも発生しないため, 呼び出し順に注意. あるいは `conflicted`パッケージも併用すると良いだろう. このパッケージの解説は『[名前空間の衝突をconflictedパッケージで防ぐ](https://uribo.hatenablog.com/entry/2018/09/29/103632)』でなされている.
[^computing]: 実際にはこういう複雑な計算に出くわしたら何も考えずに`rcpp`に切り替えて`for`文で書いたほうが効率が良い気もする.

# 結果

```{r kawaguchi, eval=run_heavy}
# Kawaguchi
moment_op <- function(theta, df){
  # theta = [beta_a, beta_k, alpha]
  z <- with(df, y_bl - theta[1] - theta[2] * k - theta[3] * (phi_lag - theta[1] - theta[2] * k))
  x <- df %>% ungroup %>% select(k, k_lag, inv_lag) %>% mutate(uni = 1) %>% as.matrix
  return(z * x)
}

set.seed(42)
fit_gmm <- gmm(g = moment_op, x = df_sample_2nd_np %>% drop_na(y, y_bl, k, k_lag, inv_lag, phi_lag),
               t0 = c(.1, .1, .1))
# computing boostrap SE
fitted_gmm <- function(model, data, beta_l = unname(fit_1st_np$xcoef[1])){
  params <- model$coefficients
  with(data, params[1] + params[2] * k + beta_l * l)
}
resid_gmm <- function(model, data, beta_l = unname(fit_1st_np$xcoef[1])){
  params <- model$coefficients
  with(data, y - params[1] + params[2] * k + beta_l * l)
}
stat_gmm <- function(data, indices, indices_, bw){
  data$y <- with(data, fitted + resid[indices])
  fit_1st_np <- npplreg(bws = bw, txdat = as.data.frame(data[, "l"]), tydat = data$y, tzdat = as.data.frame(data[, c("k", "inv")]))
  beta_l <- unname(coef(fit_1st_np)["l"])
  phi_hat <- predict(fit_1st_np, newdata = data %>% drop_na(y, k, l, inv)) %>% as.numeric - drop_na(data, y, k, l, inv)$l * beta_l
  data <- make_2nd_sample(data, beta_l, phi_hat, 1)
  fit_gmm <- gmm(g = moment_op, x = data %>% drop_na(y, y_bl, k, k_lag, inv_lag, phi_lag), t0 = c(.1, .1, .1))
  params <- unname(fit_gmm$coefficients)
  beta_a <- params[1]; beta_k <- params[2]
  rmse = sqrt(mean(with(data, y - beta_a - beta_k * k -beta_l * l)^2, na.rm = T))
  return(c(beta_a = beta_a, beta_k = beta_k, beta_l = beta_l, rmse = rmse))
}

# compute boostrap SE
set.seed(42)
boot_gmm <- boot(df_sample %>% drop_na(y, k, l) %>% mutate(., fitted = fitted_gmm(fit_gmm, .), resid = resid_gmm(fit_gmm, .)),
                 stat_gmm, R = 100, bw = bw_1st)
```
```{r comparisons}
df_result_table_kawaguchi <- tibble(
  name = c("beta_a", "beta_k", "beta_l"),
  value = c(fit_gmm$coefficients[1:2], coef(fit_1st_np)) ,
  std_err = apply(boot_gmm$t, 2, sd)[1:3]
  ) %>% mutate(model = "GMM (Kawaguchi)")

# estprod
fit_estprod <- estprod::olley_pakes(
  y ~ l | k | k + inv,
  data = df_ground_truth %>% mutate(y = y_true) %>% select(y, k, l, inv, i, t),
  id = "i", time = "t")

# OLS
fit_ols <- lm(y ~ k + l, data = df_sample)
# by plm
fit_within <- plm(y ~ k + l, data = df_sample %>% arrange(i, t),
                  index = c("i", "t"),
                  model = "within")
fit_between <- update(fit_within, model = "between")
fit_twoway <- update(fit_within, model = "within", effect = "twoways")
fit_FD <- update(fit_within, model = "fd")
fit_RE <- update(fit_within, model = "random")
# Diff-GMM by Arellano and Bond, and System GMM by Blundel and Bond
fit_pgmm <- list(
  AB_twoway = pgmm(y ~ plm::lag(k) + k + l | plm::lag(k, 2:99) + plm::lag(l, 2:99), data = df_sample, effect = "twoways"),
  AB_indivi = pgmm(y ~ plm::lag(k) + k + l | plm::lag(k, 2:99) + plm::lag(l, 2:99), data = df_sample, effect = "individual"),
  BB_twoway = pgmm(y ~ plm::lag(k) + k + l | plm::lag(k, 2:99) + plm::lag(l, 2:99), data = df_sample, effect = "twoways", transformation = "ld"),
  BB_indivi = pgmm(y ~ plm::lag(k) + k + l | plm::lag(k, 2:99) + plm::lag(l, 2:99), data = df_sample, effect = "individual", transformation = "ld")
)
df_result_table_plm <- c(list(OLS = fit_ols, Within = fit_within, Between = fit_between, Twoway = fit_twoway, FD = fit_FD, RE = fit_RE),
                         fit_pgmm) %>% map2_dfr(., names(.), ~tibble(model = .y, fit = list(.x))) %>%
  mutate(
    beta_a_value = map_dbl(fit, ~coef(.x)["(Intercept)"]),
    beta_a_value = if_else(
      is.na(beta_a_value),
      map_dbl(fit, ~with(df_sample, mean(y - coef(.x)["k"] * k + coef(.x)["l"] * l, na.rm = T))),
      beta_a_value),
    beta_a_se = map_dbl(fit, function(x){
      coefs <- summary(x, robust = T)$coefficients
      if("(Intercept)" %in% rownames(coefs)){
        coefs["(Intercept)", "Std. Error"]
      } else{
        NA
      }
    }),
    beta_l_value = map_dbl(fit, ~coef(.x)["l"]),
    beta_l_se = map_dbl(fit, ~summary(.x)$coefficients["l", "Std. Error"]),
    beta_k_value = map_dbl(fit, ~coef(.x)["k"]),
    beta_k_se = map_dbl(fit, ~summary(.x)$coefficients["k", "Std. Error"])
  )  %>% pivot_longer(cols = starts_with("beta_"),
                      names_to = c("name", ".value"), names_pattern = "(beta_[alk])_(.+)") %>% rename(std_err = se)
```

FE, RDなどの内生性を想定しないパネルデータの方法と, Arellano-Bondの階差GMMなどの動学パネルデータモデル, OPの方法, そして\@ref(kawaguchi-gmm)で詳しく紹介するKawaguchiの課題での方法のそれぞれの推定結果を図\@ref(fig:result)に示す[^failed-methods]. OPの方法は, 多項式回帰とカーネル回帰の両方を試したいところだが, カーネル回帰は既存のパッケージを使いまわせず実装にも時間がかかるので省略した.
また, 確率推定をGAMやランダムフォレストに置き換えたケースについても, 結果にほとんど違いがなかったため省略した.
Kawaguchiの課題では一般化モーメント法(GMM)で計算しており, その条件の特定は今回のデータとも合致しているが退出行動を省略しているためバイアスが発生することが予想できる(やる気があれば本来の課題との結果の違いを比較すると面白いだろう).

```{r result, fig.cap="推定結果"}
bind_rows(df_result_table_op, df_result_table_plm) %>%
  filter(name %in% c("beta_a", "beta_k", "beta_l")) %>%
  filter(!str_detect(model, "^BB_"), !str_detect(model, "^AB_"), model != "Twoway", !model %in% c("OP_RF", "OP_GAM")) %>%
  mutate(model = str_replace(model, "OP_poly", "OP (polynomial)"),
         name = factor(name, labels = c(latex2exp::TeX("$\\beta_A$"), latex2exp::TeX("$\\beta_K$"), latex2exp::TeX("$\\beta_L$")))
         ) %>%
  ggplot(aes(x = 0, y = value, ymin = value - std_err, ymax = value + std_err, group = model, color = model)) +
  geom_point(size = 4, position=position_dodge(width=0.5)) +
  geom_errorbar(width = .3, size = 1, position=position_dodge(width=0.5)) +
  geom_hline(aes(yintercept = val),
             data = tibble(name = factor(c("beta_a", "beta_k", "beta_l"),
                                         labels = c(latex2exp::TeX("$\\beta_A$"), latex2exp::TeX("$\\beta_K$"), latex2exp::TeX("$\\beta_L$"))),
                           val = c(beta_a., beta_k., beta_l.)), linetype = 2) +
  thm + theme(legend.title = element_blank(), axis.title = element_blank(),
              axis.text.x = element_blank(), axis.ticks.x = element_blank()) +
  scale_color_colorblind() +
  facet_wrap(~name, labeller = label_parsed) +
  labs(caption = "dashed lines denote the true value.")
```

それ以外の方法については, 推定された係数のバイアス(真値との差)のみを表\@ref(tab:result-table)に掲載する[^table-note]

```{r result-table}
bind_rows(df_result_table_op, df_result_table_plm, df_result_table_kawaguchi) %>% select(name, value, model) %>%
  filter(name %in% c("beta_a", "beta_k", "beta_l")) %>%
  pivot_wider(id_cols = model, names_from = name, values_from = value) %>%
  mutate(beta_a = beta_a - beta_a., beta_k = beta_k - beta_k., beta_l = beta_l - beta_l.,
         model = str_replace(model, "_(.+)", " (\\1)")) %>%
  select(model, beta_a, beta_l, beta_k) %>%
  kable(digits = 3, caption="各推定のバイアス", booktabs=T, col.names = c("Model", expression(beta[A]), expression(beta[L]), expression(beta[K])))
```

[^failed-methods]: estprod`はOP法の実装といいつつ企業の途中退出を考慮していない上, 退出のないデータに当てはめても全く違う結果が帰ってきたので**なかったことにした**. また, 今回書いたOP法の各バリエーションは結果がほとんど変わらなかったため多項式の場合のみを代表して掲載し, AB/BB推定量はあまりにも誤差が大きいため掲載を省略した.
[^table-note]: 定数項 $\beta_A$を推定できない方法については残差の平均を定数項として扱っている. また, OP方のうちGAM, Rは第1段階を部分線形モデルで推定している.

# まとめ

今回は, 因果推論と呼ばれるRubin的な介入効果の推定メソッドが経済学では誘導形と呼ばれること, そして誘導形と対比される形で構造推定と呼ばれる方法論が存在することを説明した. 今回具体例として取り上げたOlleyとPakesの研究は, 変数の内生性によって, 動学パネルデータや個別効果モデルといった従来の推定方法では生産関数を適切に推定できないことを指摘した上で, 経済モデルを想定した推定方法によってパラメータを識別したというものである. そして, 提案される計算手順をRでデモンストレートした.

今回は取り掛かった段階で準備期間が1週間を切っていたため, かなり手を抜かざるを得なかった. 以前の発表の続編ということで, 動学構造推定をRで実演したかったのだが, まず説明を書くのに時間がかかるので, 静学構造推定に, そしてRのプログラムを用意する時間もなくなってきたので実装もなし, となし崩し的に縮退していった. さらには, タイトルで「反事実分析」と書いたわりに「反事実」的な要素のあまりない研究を挙げることになってしまった.

今回挙げたOlley-Pakes や Levinsohn-Petrinの研究は10年以上前の話なので, すでに大学院, あるいは学部の授業でも取り上げられることすらあるようだ. そういう意味でもあまりおもしろみのない発表だったとは思う.

しかし, 実際に乱数データでOP法本来のやり方を試してみると, 簡単な生成ルールであっても初期値に依存して標準誤差が大きくなるなど, 意外と扱いづらいことがわかった. これは再現性の問題にも関わってくると思うので, より安定した推定方法を議論するのもおもしろいかもしれない.

# (APPENDIX) 補遺 {-}

# 数値計算上に関係する実装上の注意 {#init}

非線形最小二乗法は一般に解が一意とは限らない. Olleyらは$h_{t}:=\hat{\phi}_{t}-\beta_{K}k_{t}$として計算しており, $\beta_{A}$を除いていない. この方法では $\beta_{A},\omega_{t}$を識別できないからである(彼らの目的はTFPの計算なので, $\beta_{K},\beta_{L}$さえ識別できれば問題にならない). 例えば$\mathrm{E}\omega_{t}=0$と分かっているならば識別できるが, 今回の仮定ではそれはわからないため, 数値計算の安定のために$\beta_{A}$の減算を省略している.

また, 非線形最適化は初期値を変えると結果が大きく変わることがある. 一般的な解決方法はないが, 今回は生産関数の意味から考えて $0<\beta_{K}$という制約を与えている. もし$\beta_{K}$がゼロや負なら, 資本投入量を増やしてもまったく生産量が増えないか, むしろ減少することになる. これは通常ならありえないことだろう. `optim()`関数には `lower`というオプションがあるが, ここでは計算速度を損なわないように $\beta_{K}$のみ指数変換して計算するように修正している さらに, 初期値を$1-\hat{\beta}_{L}$とした. これは, $\beta_{K}+\beta_{L}=1$ならば生産関数が**規模に対して収穫一定**になるからである. もちろんそのようになるという仮定をしていないが, 生産関数のパラメータが極端な値にならないという前提ならば中庸な設定であり, 初期値に向いていると考えられる. この初期値に乱数でばらつきを与えて複数回の結果を確認すると, 初期値と収束した解の対応関係は図\@ref(fig:init)のようになり, 次数$q$ごとの誤差$\pm\sigma$は図\@ref(fig:beta-error)のようになった. このように, $\beta_{K}$は初期値によって結果にばらつきがあり, $\beta_{A}$は初期値に完全に依存していることがわかる. そのため,今回は各次数について100回試行し, 平均値から次数を決定し, その後再びブートストラップ法で反復計算した結果を最終的な推定値として採用することにした.

```{r init, fig.cap="解の初期値と収束先"}
plot_iterated_convergence <- function(data, max_degree = 6, size = 20){
  # plot iterated results
  data %>% filter(d <= max_degree, n <= size) %>% mutate(
    beta_a_start = map_dbl(start, ~.x[1]),
    beta_k_start = map_dbl(start, ~.x[2])
  ) %>% ggplot(aes(x = beta_a_start, y = beta_k_start, xend = beta_a, yend = beta_k, color = log(rmse))) +
    geom_segment(arrow = arrow(length = unit(.05, "npc")), size = .5) + geom_point() +
    facet_wrap(~d) + thm + theme(legend.position = "right") +
    labs(x = latex2exp::TeX("$\\beta_A$"), y = latex2exp::TeX("$\\beta_K$"))
}
plot_iterated_convergence(df_fit_2nd_poly)
```
```{r beta-error, fig.cap="次数ごとの初期値の違いによる誤差"}
plot_iterated_errorbar <- function(data){
  # plot summarised result
  skim(data %>% group_by(d)) %>% select(d, skim_variable, numeric.mean, numeric.sd) %>%
    filter(skim_variable %in% c("beta_a", "beta_k")) %>%
    mutate(variable = factor(skim_variable, labels = c(latex2exp::TeX("$\\beta_A$"), latex2exp::TeX("$\\beta_K$")))) %>%
    ggplot(aes(x = d, y =numeric.mean)) + geom_line() +
    geom_errorbar(aes(ymin = numeric.mean - numeric.sd, ymax = numeric.mean + numeric.sd)) +
    facet_wrap(~variable, scales = "free_y", labeller = label_parsed) +
    scale_x_continuous(breaks = 1:8) + thm + theme(axis.title.y = element_text(angle = 90)) + labs(x = latex2exp::TeX("$q$"), y = "estimates")
}
plot_iterated_errorbar(df_fit_2nd_poly)
```

また, 乱数データを生成するパラメータを変更すれば当てはまり易さも変わってくることに注意する. 例えば $p_{t}$の推定には$\mathit{inv},k_{t}$を使用しているが, もしこれらと$x_{t}$との相関が弱ければ, 弱相関操作変数と同様にかえってバリアンスが増加する原因になると予想できるため, 企業のデータを取れば常にOP法をそのまま適用すればいいということはありえないだろう.
このような観点からも構造推定は決まったパッケージの決まった構文を実行すれば終わりというものではなく, 各段階で仮説が正しいかを検証する必要がある.

# ブートストラップ標準誤差 {#boot}

Efron (1979, Bootstrap Methods: Another Look at the Jackknife)は再標本化によって標準誤差や分布の分位点を推定する方法をいくつも提案し, これらをブートストラップ法と総称した. ブートストラップ法はパラメトリックなものとノンパラメトリックなものに大別されるが, 今回はノンパラメトリックなブートストラップ法を利用する. 最も単純なブートストラップ法は次の通り[^boot-textbook]. $N$件のデータ$\mathcal{D}=\{z_{1},\cdots,z_{N}\}$を取得できたとして, ここから復元抽出した$N$件の標本(ブートストラップ標本)を$B$個作成する.
$$\begin{aligned}
D_{1}= & \{z_{1}^{(1)},\cdots,z_{N}^{(1)}\},\\
\vdots\\
D_{B}= & \{z_{1}^{(B)},\cdots,z_{N}^{(B)}\}\end{aligned}$$
もし, この擬似的に作られたブートストラップ標本$D_{b}$が母集団から無作為抽出された標本と同等の分布に従うならば, それぞれの標本平均$\bar{z}^{(b)}$はすべて同一の分布にしたがう. よって, 以下のように計算した標本標準偏差を標準誤差の推定値とすることができる.

$$\begin{aligned}
\hat{\mathit{SE}}(\bar{z}):= & \sqrt{\frac{1}{B}\sum_{b=1}^{B}(\bar{z}^{(b)}-\bar{z}_{B})^{2}},\\
\bar{z}_{B}:= & \frac{1}{B}\sum_{b=1}^{B}\bar{z}^{(b)}\end{aligned}$$

これが標本平均でなく回帰分析の係数に置き換わっても同じことが言える. つまり今回のOP法のように推定量の分布を解析的に導出するのが難しい場合にブートストラップ法を利用する場面がある.

しかし, 「ブートストラップ標本が母集団から無作為抽出された標本と同等の分布に従う」という前提を忘れてはならない. 上記の単純なケースではどの標本も等確率に抽出されており, 言い換えるなら母集団の分布がそのようになっていると暗黙に仮定していることになる. Efron (1979, Bootstrap Methods: Another Look at the Jackknife)は, 推定した回帰モデルの残差から誤差項の経験分布を構成し, これを誤差項$\varepsilon$の分布とみなしてサンプリングした乱数$\hat{\varepsilon}_{i}^{\ast}$とモデルの予測値の和
$$\hat{y}_{t}^{\ast}:=\hat{y}_{t}+\hat{\varepsilon}_{t}^{\ast}$$
を生成することを提案している.

標本からの計算のため, 大数の法則や中心極限定理からの類推で直観的に$B$が大きいほど推定の精度がよくなると予想できる. しかしながら, 具体的にどれくらいなら十分であるかを示す理論は存在しないようだ. ネット上では具体的な数字を挙げる例も見られるが, 具体的な根拠は挙げられていないことが多い[^boot-size]. これに関しては [Rule of thumb for number of bootstrap samples - Cross Validated](https://stats.stackexchange.com/questions/86040/rule-of-thumb-for-number-of-bootstrap-samples) のAdamOの回答が参考になる. いわく, 十分な回数は問題に応じて変わる(例えばコーシー分布に従うデータならば何万, 何千万回やろうとそも存在しない平均を求めるのに十分ではない)ため, ブートストラップ計算の結果をヒストグラムに表し分布が適切な形になっているかを確認せよ, ということである. とはいえ, 彼の回答の追加コメントでも指摘されるように, 多くの場合は回数が多いほど精度が増すはずなので, 例えば先行研究と比較できるようにいったん同じ回数で計算した上でヒストグラムを確認し, もしうまくいってないなら回数を増やす (またはバグを疑う), といった方法が現実的ではないかと考える.

ブートストラップ法の計算に関するRのパッケージは`simpleboot`と`boot`パッケージの2つが代表的である[^boot-references].`  simpleboot `は名前の通り簡単なブートストラップ法の計算だけに限定したパッケージで, OP法のような特殊なモデルには対応していないから`boot::boot()`を使う. ただし, `np`パッケージによるカーネルベースの推定方法はただでさえ時間がかかるため, 今回は1段階目を多項式近似でやることにした.

`boot()`の使い方はパッケージのヘルプを見ればすぐ分かるが, 一応簡単に解説しておく. 第1引数`data`にデータを, 第2引数`statistic`に計算したい統計量(のベクトル)を返す関数を与える. `boot()`は`data`をリサンプリングしてブートストラップ標本を作り, それぞれを`statistic`に与えてブートストラップ推定値を計算する. `statistic`に与える関数の第1引数には`data`が, 第2引数には生成ブートストラップ標本が`data`のどこかを表すインデックスベクトルが与えられる. さらに関数ヘルプには補外データに対してブートストラップ法を適用する例が掲載されている. この場合, 予測値の計算にももう1つインデックスベクトルが必要になる. これが第3引数である. 第4引数以降は, ユーザーが自由に定義できる引数である. そのため, 既に書いたように回帰モデルに対してノンパラメトリックブートストラップ法を適用する場合は, 予め残差を計算して入力データに含めておく必要がある.

今回はのように書いた. 回帰モデルのブートストラップ統計量の計算のため, `data`には通常の推定に必要な変数に加え, ブートストラップ標本を生成するための$\hat{y}_{i,t},\hat{\varepsilon}_{i,t}$の列を追加する必要がある.

今回`statistic`が出力すべき統計量は3つの係数パラメータ$\beta_{A},\beta_{K},\beta_{L}$だが,
確認のため各段階で推定したモデルの当てはまりについても出力するようにしている[^boot-unname].

```{r computing-boot, eval=run_heavy, echo=T}
op_params_poly <- list(beta_k = exp(fit_2nd_poly$par[2]), beta_l = unname(coef(fit_1st_poly)["l"]))

estimate_op_ordinary <- function(data, degree_1, degree_p, degree_2nd){
  fit_1st <- lm(y ~ l + poly(k, inv, degree = degree_1), data = data, na.action = na.omit)
  beta_l <- unname(coef(fit_1st)["l"])
  rmse_1st <- sqrt(mean(resid(fit_1st)^2))
  fit_exit <- glm(x ~ l + poly(k_lag, inv_lag, degree = degree_p), data = data %>% drop_na(k_lag, inv_lag), family = binomial(link = "probit"))
  std_Brier <- std_Brier(fit_exit$data$x, fitted(fit_exit))
  df_2nd <- make_2nd_sample(
    data, beta_l,
    phi = predict(fit_1st, newdata = data) - beta_l * data$l,
    p_x = predict(fit_exit, newdata = data, type = "response")
  )
  fit_2nd <- optim(
    par = c(rnorm(n = 1, sd = .5), log(ifelse(1 - beta_l <=0, .5, 1 -beta_l)) + rnorm(n = 1, sd = .3), rnorm(n = sum(1:degree_2nd + 1), sd = .5)),
    fn = obj_op_2nd, data = df_2nd, degree = degree_2nd)
  beta_a <- mean(with(df_2nd, y_bl - exp(fit_2nd$par[2]) * k), na.rm = T)
  return(c(beta_a = beta_a, beta_l = beta_l, beta_k = unname(exp(fit_2nd$par[2])), rmse_1st = rmse_1st, std_Brier = std_Brier, rmse_2nd = sqrt(fit_2nd$value)))
}

get_stat_boot <- function(data, indices, indices_, estimate, params){
  # estimate = estimate function
  # params = model hyper parameter (e.g., degrees of polynomials)
  data$y <- with(data, fitted + resid[indices])
  do.call(estimate, args = c(list(data = data), params)) }

boot_ordinary <- df_sample_2nd_poly %>%
  mutate(., fitted = if_else(is.na(y), NA_real_, do.call(get_fitted, args = c(list(data = .), op_params_poly))),
         resid = y - fitted) %>%
  boot(., get_stat_boot, R = 100, strata = .$x, estimate = estimate_op_ordinary, params = list(degree_1 = 3, degree_p = 2, degree_2nd = 2))
```

なお, 既に書いたようにOP法では$\beta_{A}$を識別できないが, 他の方法との比較のため$\hat{\beta}_{A}:=(NT)^{-1}\sum_{i,t}(y_{i,t}-\hat{\beta}_{L}l_{i,t}-\hat{\beta}_{K}k_{i,t})$として計算した. これは本来の仮定に加えて$\mathrm{E}\omega_{t}=0$が成り立ってなければ適切な推定量とは言えないが, 今回の乱数データでは$\omega_{t}$が平均ゼロの定常分布に収束するような方法で生成している.

引数`R`はブートストラップ標本の生成数である. 本文での解説に沿って100に設定し, 各パラメータのヒストグラムが不自然でないか確認した. `strata`は層別抽出をするためのインデックスである. 今回は$y_{t}$に欠損があり, OP法は欠損していないデータだけで推定しているから, ブートストラップ標本$\hat{y}_{t}^{\ast}$も本来のデータの欠損メカニズムを再現する必要がある. ここでは`strata`に元のデータの欠損判定$x_{t}$を与えることでこれを表現している[^bootstrap-missing]. また, `boot.ci()`で`boot()`の結果から信頼区間を計算できるが, 今回は利用しない.

[^boot-textbook]: 日本語で書かれ, かつネット上で公開されているブートストラップ法の基本的かつ有用な解説は少ない. 標準誤差には言及していないが, Rを使った簡単な実演が『[ブートストラップ入門](http://tarohmaru.web.fc2.com/R/bootstrapping.html)』で紹介されている.
    pdfファイルが不鮮明だが, 汪, 大内, 景, & 田栗 (1992, ブートストラップ法 最近までの発展と今後の展望 第5節)で最も詳しい説明がなされている.
    これらの記述を総合すればだいたいのことはわかるが, いちおうここで要約しておく. それ以外にもいくつかの研究論文がみつかるが, 回帰モデルのブートストラップ標準誤差を求める方法の解説としては使いづらい.
    Efronの論文は半世紀も前のものなので, 日本語で書かれた有用な教科書ならば多く存在する. ただし私は金, 汪, & 桜井 (2011, *ブートストラップ入門*)のものしか読んでいない.
[^boot-size]: 例えば標準誤差の計算なら100程度, 分位点の計算なら1000程度, という記述がよく見られる. この数値の根拠は @efron1979Bootstrap の論文に掲載されている実験でそのように設定されていたからではないかと思う. しかしEfronはこの数字の根拠を述べていない.
[^boot-references]:  金明哲『[Rとブートストラップ](https://www1.doshisha.ac.jp/~mjin/R/Chap_44/44.html)』と奥村晴彦『[ブートストラップ](https://oku.edu.mie-u.ac.jp/~okumura/stat/bootstrap.html)』はいずれもこれらのRパッケージを使った計算方法も含めて言及している. 『[ブートストラップのためのbootパッケージ](http://ryamada22.hatenablog.jp/entry/20160108/1452219319)』にも`boot`パッケージの言及がある.
[^boot-unname]: サンプルプログラムでは返り値に名前を付けているが, `boot()`は名前の情報を保存してくれないようだ
[^bootstrap-missing]: 欠損値を含む場合のブートストラップ法に詳しくないが, $y_{t}$に対して$x_{t}$は先決(predetermined)変数だから, 単純に$x_{t}$の層化で欠損を条件付けるだけで十分だと思う.

# kawaguchiの課題についての補足 {#kawaguchi-gmm}

Olleyらの方法に倣えば非線形最小二乗法として係数$\beta_{A},\beta_{K}$を求める必要があるが, 課題では退出行動を省略し, $\omega_{t}$の構造もシンプルなためOlleyらのような複雑な式の導出はなく, **非線形一般化モーメント法** (非線形GMM)で推定することを課している. GMMの解説はすくなく, 非線形GMMとなるとなおさらなので[^nonlinear-GMM], ここで少し脱線して課題では何をやっているのかを書いておく.

練習用データは$\omega_{t}$をAR(1)過程の乱数として生成しているので, 課題では係数$\alpha$は未知であるもののAR(1)であることは分かっているという前提である. つまり, 以下のように仮定している[^kawaguchi-specification]ので,
$$\begin{aligned}
h(\omega_{t+1})= & \alpha\omega_{t}+\nu_{t},\\
\nu_{t}\sim & \mathcal{N}(0,\sigma_{\nu}^{2})\forall t\end{aligned}$$

$$\begin{aligned}
\hat{\xi}_{t+1}:= & \omega_{t+1}-\alpha\left(\hat{\phi}(k_{t},\mathit{inv}_{t})-\beta_{A}-\beta_{K}k_{t+1}\right)\end{aligned}$$
とすると,
$$\begin{aligned}
y_{t+1}= & \beta_{L}l_{t+1}+\beta_{A}+\beta_{K}k_{t+1}+\alpha\left(\hat{\phi}(k_{t},\mathit{inv}_{t})-\beta_{A}-\beta_{K}k_{t+1}\right)+\hat{\xi}_{t+1}+\varepsilon_{t+1}\end{aligned}$$
となり, これを変形すると以下のようになる.
$$\begin{aligned}
\hat{\xi}_{t+1}+\varepsilon_{t+1}= & y_{t+1}-\hat{\beta}_{L}l_{t+1}-\beta_{A}-\beta_{K}k_{t+1}-\alpha\left(\hat{\phi}(k_{t},\mathit{inv}_{t})-\beta_{A}-\beta_{K}k_{t+1}\right)\end{aligned}$$
既に書いた仮定では $\hat{\xi}_{t+1}+\varepsilon_{t+1}$は同時点のいくつかの変数と相関する可能性が残るが同時点の$k_{t+1}$および1時点前の$k_{t},\mathit{inv}_{i,t}$と無相関なので以下のような**モーメント条件**が成り立つ.
$$\begin{aligned}
\mathrm{E}(\hat{\xi}_{t+1}+\varepsilon_{t+1})\begin{bmatrix}k_{t+1}\\
k_{t}\\
\mathit{inv}_{t}
\end{bmatrix}= & \mathbf{0}\end{aligned}$$

$\hat{\xi}_{t}+\varepsilon_{t}$にはパラメータに対して非線形な項が含まれるので非線形モデルということになる. 線形モデルの場合は単にGMMと呼ばれ, 今回のような**非線形モーメント条件**を特定した場合を**非線形GMM**と呼び, ニュートン法などの非線形最適化の方法で計算することになる. なお計算には新たに$y_{t}-\hat{\beta}_{L}l_{t}$,$\hat{\phi}_{t}$, $k_{t},\mathit{inv}_{t}$のラグ項が必要になるが, これらは既にで作成している.

RでGMM推定法を計算するパッケージには `gmm`, `plm`がある[^plm-materials]. 後者の`plm`はGMMだけでなく, FE/REモデルなどパネルデータを想定したモデルの計算用関数が用意されている. 今回もパネルデータを利用しているが, 個別効果を想定していないため`plm`を必ず使わなければならないわけではない. 実際のところ, `plm::pgmm()`でできるGMMは既に少しだけ言及したArellano-Bond/Blundell-Bondの動学パネルデータモデルの推定だけであるので, 今回のような**非線形GMMには使えない**. 一方で`gmm`パッケージは非線形GMMにも対応しているので, 今回はこちらを使う.

また, GMMを使う場合も標準誤差はブートストラップ法で計算する必要がある. しかしカーネル回帰は非常に時間がかかるため, 100回も1000回も繰り返すのは現実的ではない. 今回時間がかかるのは主に最適バンド幅を計算する`npplregbw()`の実行なので, 最初の計算で得たバンド幅を全てのブートストラップ標本で共用することにした. しかし, `np`パッケージは**引数に対する挙動がかなり意味不明で混乱を招く**. 所与のバンド幅でデータを変えて再計算したい場合の方法が不明瞭である. いろいろ試したが, `data`に与えるだけではバンド幅計算時の結果がそのまま出力されるようだ. `txdat`, `tydat`, `tzdat`にそれぞれ新しい数値を与える必要があるらしい.

```{r, eval=F, echo=T}
npplreg(bws = bw,
        txdat = as.data.frame(df_sample[, "l"]),
        tydat = df_sample$y,
        tzdat = as.data.frame(df_sample[, c("k", "inv")]))
```


[^nonlinear-GMM]: 非線形GMMは @hayashi2000Econometrics を始め大学院向け標準レベルの教科書なら必ずと言っていいほど載っているが, 日本語の教科書では線形GMMまでしか書いていないことが多い. 私が以前書いたブログ記事でも肝心の非線形GMMの説明がなかった. どんなモーメント条件であっても求められるわけではなく, ニュートン法は目的関数が凹関数でなければ一意な解を得られない. 実際には一致性や漸近分布を保証するためにさらにいくつかの十分条件を考える必要がある. それらの議論も詳しくは教科書を参照してほしい.
[^kawaguchi-specification]: 課題文にも書いてあるように, 本来は$\omega_{t}$の決定メカニズムを知ることは難しいため現実の問題に対してこのような強い仮定を置くことは好ましくない.
[^plm-materials]: `plm`の使い方は開発者による解説[Estimation of error components models with the plm function](https://cran.r-project.org/web/packages/plm/vignettes/plmFunction.html)が参考になるだろう.
    一方で日本語の情報では慶應義塾大の長倉大輔氏?作のpdf『[Rによるパネルデータモデルの推定](http://user.keio.ac.jp/~nagakura/R/R_panel.pdf)』があるが, それ以外では参考になるものは少ない. また, 公式ドキュメントを見ても `plm()`がデフォルトでどのように個体・時間インデックスを認識するのかがよくわからない. 念のため`index=`を指定したほうがいいだろう.

## `gmm`パッケージの解説

`gmm`の使い方はヘルプを見れば十分なのだが, タダで見られてかつ日本語のGMMの解説は少なく, 非線形GMMの解説は一層少ない. そういうものを求める人のためにここで少しだけ解説しておく. まず, 線形の場合は`lm()`などと同じように`formula`オブジェクトでモーメント条件を指定できる.
例えば,
$$\begin{aligned}
\mathrm{E}(y_{i}-\alpha-\beta x_{i})\begin{bmatrix}z_{i,1}\\
z_{i,2}\\
z_{i,3}
\end{bmatrix}= & \mathbf{0}\end{aligned}$$
のようなモーメント条件をもとにGMM推定したい場合は,

```{r, eval=F, echo=T}
gmm(y~ 1 + x| z1 + z2 + z3, x = data)
```

となる[^formula-syntax].
`data` はデータフレームまたは`matrix`で与える.

非線形モデルの場合はこのようにモデルを`formula`で表現できないため, 代わりに標本モーメントを返す関数を与える. この関数関数は第1, 第2引数にそれぞれパラメータと入力データを与えて$N\times q$行列を返すものが要求される. $N$は入力データの件数, $q$はモーメント式の本数である. つまり上記の期待値の部分で,
$$\begin{aligned}
m_{i}(\alpha,\beta;x_{i}):= & (y_{i}-\alpha-\beta x_{i})\begin{bmatrix}z_{i,1}\\
z_{i,2}\\
z_{i,3}
\end{bmatrix}\end{aligned}$$
という部分であり, 返す行列は
$$\begin{aligned}
\begin{bmatrix}m_{1}z_{1,1} & m_{1}z_{1,2} & m_{1}z_{1,3}\\
m_{2}z_{2,1} & m_{2}z_{2,2} & m_{2}z_{2,3}\\
\vdots & \vdots & \vdots\\
m_{N}z_{N,1} & m_{N}z_{N,2} & m_{Z}z_{N,3}
\end{bmatrix}\end{aligned}$$
という形になる.

非線形GMMとは$m_{i}(\theta,x_{i})$の部分について, 例えば$m_{i}(\theta,x_{i}):=(y_{i}-\alpha-\gamma\beta x_{i})\begin{bmatrix}z_{i,1} & \cdots & z_{i,3}\end{bmatrix}^{\top}$のような非線形関数の場合を指す.
この\(m_{i}\)の標本平均の重み付きノルム$F_{n}$を目的関数として最小化するのがGMMである[^GMM-weight].
$$\begin{aligned}
F_{n}(\alpha,\beta)= & \left[\frac{1}{N}\sum_{i=1}^{N}m_{i}(\theta,x_{i})\right]^{\top}\hat{\mathbf{W}}^{-1}\left[\frac{1}{N}\sum_{i=1}^{N}m_{i}(\theta,x_{i})\right]\end{aligned}$$

非線形GMMはニュートン法でも計算できるため, 目的関数$F_{n}(\cdots)$を計算する関数さえ作れば, 係数だけなら`gmm`パッケージを使わなくとも`optim()`で簡単に推定できる.
そこで, 念のため`optim()`も使って検算するのも良いだろう.

[^formula-syntax]: `formula`の構文はデフォルトで定数項を加えるため, `1` は必須ではない. そもそも`formula`の構文が分からない場合は 『[71. 回帰分析と重回帰分析 - R-Source](http://cse.naro.affrc.go.jp/takezawa/r-tips/r/71.html)』や私が以前書いた『[予測モデルを作るには formula を活用せよ](http://ill-identified.hatenablog.com/entry/2017/04/30/004258)』などを参照
[^GMM-weight]: 典型的なGMMでは重み行列$\hat{\mathbf{W}}$に共分散行列の推定値を与えるが, 課題では簡略化のため単位行列を与えるよう指示がある. 単位行列の場合, 線形モデルでいう2段階最小二乗法 (2SLS) と等価な計算になる. 重み行列の役割は主に標準誤差の縮小なのでブートストラップ法で計算する今回のケースではさほど重要ではない.